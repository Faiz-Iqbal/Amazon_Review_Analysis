{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sentiment Analysis on Amazon Reviews ðŸ“Š</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The rapid growth of e-commerce, accelerated significantly during and after the COVID-19 pandemic, has reshaped consumer purchasing behaviors for both essential and non-essential goods. This shift has resulted in an overwhelming increase in online customer reviews, offering businesses a wealth of insights into customer satisfaction, product performance, and potential areas for improvement. However, the sheer volume of these reviews makes manual analysis infeasible for organizations striving to understand and act on customer sentiments effectively.\n",
    "Sentiment analysis has emerged as an essential solution, leveraging Natural Language Processing (NLP) and machine learning techniques to automatically identify and classify opinions expressed in text. This research explores the application of these techniques to analyze e-commerce reviews, aiming to uncover actionable insights at scale. By automating sentiment analysis, businesses can enhance customer experiences, personalize offerings, and make informed, data-driven decisions that align with evolving customer preferences. This study not only addresses the challenges of large-scale sentiment analysis but also highlights its transformative potential for improving business strategies in the dynamic e-commerce landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset used in this project is titled **Amazon Product Reviews** and was sourced from both Kaggle and the University of San Diegoâ€™s website. It is a publicly available dataset under the **CC0 1.0 Universal license**, which means it is free to use, share, and adapt without legal restrictions. The dataset can be accessed through [this Kaggle link](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews/data).\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset comprises the following fields:\n",
    "\n",
    "1. **Id**: A unique identifier for each review entry.\n",
    "2. **ProductId**: A unique identifier for the product being reviewed.\n",
    "3. **UserId**: A unique identifier for the user who submitted the review.\n",
    "4. **ProfileName**: The name of the user who submitted the review.\n",
    "5. **HelpfulnessNumerator**: The number of users who found the review helpful.\n",
    "6. **HelpfulnessDenominator**: The total number of users who rated the helpfulness of the review.\n",
    "7. **Score**: The rating provided by the user, typically on a scale of 1 to 5.\n",
    "8. **Time**: A timestamp representing when the review was submitted.\n",
    "9. **Summary**: A short title or summary of the review.\n",
    "10. **Text**: The full review text.\n",
    "\n",
    "### Data Preprocessing and Ethical Considerations:\n",
    "\n",
    "For this project, the **UserId** and **ProfileName** columns will be dropped from the dataset. This decision is made to ensure that no personal identifiers are used, thus maintaining ethical standards and adhering to data privacy principles. Removing these fields ensures that the dataset is ethically cleared for analysis while retaining all necessary information for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Research Questions to be Addressed\n",
    "\n",
    "- **How accurately can various machine learning models classify sentiment in e-commerce reviews?**\n",
    "- **How do different text preprocessing techniques impact the performance of sentiment classification models?**\n",
    "- **How do various feature extraction methods affect the accuracy of sentiment classification?**\n",
    "- **How do different machine learning models compare in terms of performance when classifying sentiment in e-commerce reviews?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need the following installed:\n",
    "- `pip install pandas`\n",
    "- `pip install numpy`\n",
    "- `pip install seaborn`\n",
    "- `pip install matplotlib`\n",
    "- `pip install scikit-learn`\n",
    "- `pip install nltk`\n",
    "- `pip install textblob`\n",
    "- `pip install wordcloud`\n",
    "- `pip install beautifulsoup4`\n",
    "- `pip install emoji`\n",
    "- `pip install contractions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local installation please uncomment the following and run this code block\n",
    "\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install nltk\n",
    "# %pip install textblob\n",
    "# %pip install wordcloud\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install emoji\n",
    "# %pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning and Text Vectorization\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Word Clouds\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Miscellaneous\n",
    "import collections\n",
    "import emoji\n",
    "import contractions\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading NLTK resources\n",
    "# Please uncomment the following line if you haven't downloaded the NLTK resources:\n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data is data where all reviews (1 star to 5 star) are taken in equal proportion to avoid overfitting or underfitting\n",
    "# 25000 Records of each star rating is taken\n",
    "df_balancedData = pd.read_csv('Datasets/balanced_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_balancedData['Score'].value_counts().plot(\n",
    "    kind='pie', \n",
    "    autopct='%.0f%%', \n",
    "    startangle=90,  # Rotate the pie chart for better orientation\n",
    "    colors=plt.cm.Paired.colors  # Add distinct colors for each segment\n",
    ")\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Data Distribution of Scores', fontsize=16)\n",
    "plt.ylabel('')  # Remove the default y-axis label for better aesthetics\n",
    "plt.legend(title='Scores', loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine all text from the 'Review' column into a single string\n",
    "all_reviews = \" \".join(df_balancedData[\"Text\"])\n",
    "\n",
    "# Generate the Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words, \n",
    "    width=800, \n",
    "    height=800, \n",
    "    background_color='white', \n",
    "    min_font_size=10\n",
    ").generate(all_reviews)\n",
    "\n",
    "# Plot the Word Cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_data = df_balancedData.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Generate the heatmap\n",
    "heatmap = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    vmin=-1, vmax=1, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "\n",
    "# Customize the title\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 16, 'fontweight': 'bold'}, pad=20)\n",
    "\n",
    "# Rotate x and y axis labels for clarity\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Tighten the layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Unnecesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text']]\n",
    "# Display the updated DataFrame\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CleanEmojis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply emoji removal to the 'Text' column of the DataFrame\n",
    "        tqdm.pandas(desc=\"Removing emojis\")  # Add progress bar for this step\n",
    "        X['Text'] = X['Text'].progress_apply(lambda review: emoji.demojize(review))\n",
    "        return X\n",
    "\n",
    "# Apply the transformer to the DataFrame\n",
    "cleaner = CleanEmojis()\n",
    "df_balancedData_cleaned = cleaner.transform(df_balancedData)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_balancedData_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Removing Special Characters, URLS, HTML etc. and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StringProcessing class\n",
    "class StringProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for review in X:\n",
    "            # Converting to lowercase\n",
    "            review = review.lower()\n",
    "            # Substituting ampersand signs to 'and'\n",
    "            review = re.sub(r\"&amp\", \"and\", review)\n",
    "            review = re.sub(r\"&\", \"and\", review)\n",
    "            # Removing @ mentions and hashtags\n",
    "            review = re.sub(r\"[@#][^\\s]+\", \" \", review)\n",
    "            # Removing URLs\n",
    "            review = re.sub(r\"(http\\S+)|(www\\S+)\", \"\", review)\n",
    "            # Removing HTML tags\n",
    "            review = re.sub(r\"<[^<]+?>\", \" \", review)\n",
    "            # Cleaning emojis\n",
    "            review = re.sub(r\":\", \" \", review)\n",
    "            review = review.replace(\"_\", \" \")\n",
    "            # Removing special characters\n",
    "            review = re.sub(r\"[^a-z0-9'â€™ ]\", \" \", review)\n",
    "            # Removing extra whitespace\n",
    "            review = re.sub(r\"\\s+\", \" \", review)\n",
    "            filtered_tweets.append(review)\n",
    "        return filtered_tweets\n",
    "\n",
    "# Create an instance of the StringProcessing transformer\n",
    "string_processor = StringProcessing()\n",
    "\n",
    "# Apply the transformation to the 'Text' column and store the result in a new 'Text' column\n",
    "df_balancedData.loc[:, 'Text'] = string_processor.transform(df_balancedData['Text'])\n",
    "\n",
    "# Display the updated DataFrame with the new 'Review' column\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed Text into New column 'Review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text column is not needed, so its replaced with the preprocesses 'Review' column\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text']]\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to substitute contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_short_contractions(short_contractions):\n",
    "    short_contractions_dict = {}\n",
    "    # Sort the contractions alphabetically\n",
    "    for key, value in sorted(short_contractions.items()):\n",
    "        # Remove apostrophes from the contraction keys\n",
    "        new_key = key.replace(\"'\", \"\")\n",
    "        short_contractions_dict[new_key] = value\n",
    "    return short_contractions_dict\n",
    "\n",
    "# (Maarten et al., 2013)\n",
    "short_contractions = {\n",
    "    \"i'd\": \"i would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnâ€™t\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havenâ€™t\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "short_contractions_dict = collect_short_contractions(short_contractions)\n",
    "short_contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandContractions(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        expanded_reviews = []\n",
    "        # Wrap X in tqdm for a progress bar\n",
    "        for review in tqdm(X, desc=\"Expanding Contractions\"):\n",
    "            expanded_reviews.append(contractions.fix(review))\n",
    "        return expanded_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to handle Slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_slangs(slangs):\n",
    "    slang_dict = {}\n",
    "    for line in slangs.strip().split('\\n'):\n",
    "        key, value = line.lower().strip().split(': ', 1)  # Added limit to split on first ':'\n",
    "        slang_dict[key] = value\n",
    "\n",
    "    # Sort the dictionary by keys alphabetically\n",
    "    return dict(sorted(slang_dict.items()))\n",
    "\n",
    "# (SlickText, 2023)\n",
    "slangs = '''\n",
    "  ROFL: Rolling on the floor laughing\n",
    "  STFU: Shut up\n",
    "  ICYMI: In case you missed it\n",
    "  TL;DR: Too long, didnâ€™t read\n",
    "  TMI: Too much information\n",
    "  AFAIK: As far as I know\n",
    "  LMK: Let me know\n",
    "  NVM: Nevermind\n",
    "  FTW: For the win\n",
    "  BYOB: Bring your own beer\n",
    "  BOGO: Buy one get one\n",
    "  JK: Just kidding\n",
    "  TBH: To be honest\n",
    "  TBF: To be frank\n",
    "  RN: Right now\n",
    "  BRB: Be right back\n",
    "  BTW: By the way\n",
    "  GG: Good game\n",
    "  IRL: In real life\n",
    "  LOL: Laugh out loud\n",
    "  SMH: Shaking my head\n",
    "  NGL: Not gonna lie\n",
    "  IKR: I know right\n",
    "  TTYL: Talk to you later\n",
    "  IMO: In my opinion\n",
    "  WYD: What are you doing?\n",
    "  IDK: I donâ€™t know\n",
    "  IDC: I donâ€™t care\n",
    "  IDGAF: I donâ€™t care\n",
    "  TBA: To be announced\n",
    "  TBD: To be decided\n",
    "  AFK: Away from keyboard\n",
    "  IYKYK: If you know you know\n",
    "  B4: Before\n",
    "  FOMO: Fear of missing out\n",
    "  GTG: Got to go\n",
    "  G2G: Got to go\n",
    "  H8: Hate\n",
    "  LMAO: Laughing my ass off\n",
    "  IYKWIM: If you know what I mean\n",
    "  MYOB: Mind your own business\n",
    "  POV: Point of view\n",
    "  HBD: Happy birthday\n",
    "  WYSIWYG: What you see is what you get\n",
    "  FWIF: For what itâ€™s worth\n",
    "  TW: Trigger warning\n",
    "  EOD: End of day\n",
    "  FAQ: Frequently asked question\n",
    "  AKA: Also known as\n",
    "  ASAP: As soon as possible\n",
    "  DIY: Do it yourself\n",
    "  NP: No problem\n",
    "  U: you\n",
    "  R: are\n",
    "  PLS: please\n",
    "  WYD: what are you doing\n",
    "  N/A: Not applicable\n",
    "  K: Okay\n",
    "  WUT: what\n",
    "  FYI: For your information\n",
    "  NSFW: Not safe for work\n",
    "  WFH: Work from home\n",
    "  OMW: On my way\n",
    "  DM: Direct message\n",
    "  FB: Facebook\n",
    "  IG: Instagram\n",
    "  YT: YouTube\n",
    "  QOTD: Quote of the day\n",
    "  OOTD: Outfit of the day\n",
    "  AMA: Ask me anything\n",
    "  HMU: Hit me up\n",
    "  ILY: I love you\n",
    "  BF: Boyfriend\n",
    "  GF: Girlfriend\n",
    "  BAE: Before anyone else\n",
    "  LYSM: Love you so much\n",
    "  PDA: Public display of affection\n",
    "  XOXO: Hugs and kisses\n",
    "  LOML: Love of my life\n",
    "  THX: thanks\n",
    "  V: very\n",
    "  OMG: Oh My God\n",
    "'''\n",
    "\n",
    "slang_dict = collect_slangs(slangs)\n",
    "slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ReplaceSlangs(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_reviews = []\n",
    "        for review in tqdm(X, desc=\"Replacing Slangs\"):\n",
    "            # Replace slang\n",
    "            for key, value in slang_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Replace short contractions\n",
    "            for key, value in short_contractions_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            filtered_reviews.append(review)\n",
    "        return filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for tweet in tqdm(X, desc=\"Tokenizing\"):\n",
    "            filtered_tweets.append(nltk.word_tokenize(tweet))\n",
    "        return filtered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to remove stopwords\n",
    "**There are certain words in the built-in NLTK stop word list that if we removed, would change the sentiment of the text. Therefore, we decided to modify the original list by removing certain words like \"no\", \"against\", \"above\" etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopwords(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        sentiment_stopwords = [\n",
    "            'against', 'above', 'below', 'up', 'down', 'over', 'under', 'no', 'nor', 'not',\n",
    "            \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "            'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "            'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "            'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "        ]\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for word in sentiment_stopwords:\n",
    "            stop_words.remove(word)\n",
    "        \n",
    "        processed_reviews = []\n",
    "        for review in tqdm(X, desc=\"Removing Stopwords\"):\n",
    "            tokens_without_stopwords = [word for word in review if word not in stop_words]\n",
    "            processed_reviews.append(tokens_without_stopwords)\n",
    "        \n",
    "        return processed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_reviews = []\n",
    "\n",
    "        def get_wordnet_pos(pos_tag):\n",
    "            if pos_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif pos_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif pos_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif pos_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN\n",
    "        \n",
    "        # Wrap X in tqdm\n",
    "        for review in tqdm(X, desc=\"Lemmatizing\"):\n",
    "            pos_tags = nltk.pos_tag(review)\n",
    "            lemmatized_tokens = [\n",
    "                lemmatizer.lemmatize(token, get_wordnet_pos(tag)) \n",
    "                for token, tag in pos_tags\n",
    "            ]\n",
    "            lemmatized_reviews.append(lemmatized_tokens)\n",
    "        \n",
    "        return lemmatized_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic cleaning after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.remove_digits = re.compile(r'\\d+')\n",
    "        self.remove_single_letters = re.compile(r'\\b\\w{1}\\b')\n",
    "        self.remove_apostrophes = re.compile(r\"'\")\n",
    "        self.remove_href = \"href\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        cleaned_reviews = []\n",
    "        \n",
    "        for review in tqdm(X, desc=\"Final Cleaning\"):\n",
    "            cleaned_words = [\n",
    "                self.clean_word(word)\n",
    "                for word in review\n",
    "                if word and word != self.remove_href\n",
    "            ]\n",
    "            cleaned_reviews.append(cleaned_words)\n",
    "        \n",
    "        return cleaned_reviews\n",
    "\n",
    "    def clean_word(self, word):\n",
    "        # Removing digits\n",
    "        word = self.remove_digits.sub(\"\", word)\n",
    "        # Removing single letters\n",
    "        word = self.remove_single_letters.sub(\"\", word)\n",
    "        # Removing apostrophes\n",
    "        word = self.remove_apostrophes.sub(\"\", word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews_frame):\n",
    "    # Define the list of preprocessing steps as tuples\n",
    "    # Each tuple contains a step name (for identification) and an instance of the corresponding transformer\n",
    "    preprocessing_steps = [\n",
    "        ('expand_contractions', ExpandContractions()),  # Step to expand contractions like \"don't\" -> \"do not\"\n",
    "        ('replace_slangs', ReplaceSlangs()),         # Step to replace slang words with their full form\n",
    "        ('tokenize', Tokenize()),                    # Step to tokenize the text (split into words)\n",
    "        ('remove_stopwords', RemoveStopwords()),     # Step to remove common stopwords\n",
    "        ('lemmatize', Lemmatize()),                  # Step to lemmatize words (reduce to root form)\n",
    "        ('final_clean', FinalClean())                # Final cleaning step (removing unwanted characters, etc.)\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline object using the preprocessing steps defined above\n",
    "    # The Pipeline applies each step sequentially to the data\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "    \n",
    "    # Apply the pipeline to the 'Text' column of the DataFrame\n",
    "    # This will preprocess the 'Text' data column by applying each step defined in the pipeline\n",
    "    preprocessed_reviews = preprocessing_pipeline.fit_transform(reviews_frame['Text'])\n",
    "    \n",
    "    # Convert the list of tokenized words into strings (space-separated)\n",
    "    # List comprehension is used to join each list of tokens into a single string for each review\n",
    "    # Each review is a list of words, so ' '.join(review) joins them back into a single string\n",
    "    preprocessed_strings = [' '.join(review) for review in preprocessed_reviews]\n",
    "\n",
    "    # Add the preprocessed reviews as a new column to the original DataFrame\n",
    "    # This new column will contain the cleaned and processed review text\n",
    "    reviews_frame['Preprocessed_Review'] = preprocessed_strings\n",
    "\n",
    "    # Return the DataFrame with the added preprocessed reviews\n",
    "    return reviews_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_reviews(df_balancedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Method 1: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Here, we run each model separatelyâ€”each with its own hyperparameter tuning and 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Tools\n",
    "# Import necessary modules for the models and GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_balanceddata = df_balancedData.copy()\n",
    "df_tfidf_balanceddata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Sentiment to 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#   Map 5-Star Ratings to 3 Classes\n",
    "#########################################\n",
    "def map_to_3_classes(score):\n",
    "    if score in [1, 2]:\n",
    "        return 'negative'\n",
    "    elif score == 3:\n",
    "        return 'neutral'\n",
    "    else:  # 4 or 5\n",
    "        return 'positive'\n",
    "\n",
    "df_tfidf_balanceddata['Sentiment'] = df_tfidf_balanceddata['Score'].apply(map_to_3_classes)\n",
    "\n",
    "# Define Features (Preprocessed Text) and Target (Sentiment)\n",
    "X = df_tfidf_balanceddata['Preprocessed_Review']\n",
    "y = df_tfidf_balanceddata['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_balanceddata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,      # 70 train 30 test\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 2. TF-IDF Transformation (Once Only)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Can be varied\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create a dictionary to store the final results from each model (optional)\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 1: DECISION TREE\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 1: Decision Tree\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=dt_param_grid,\n",
    "    scoring='accuracy',  \n",
    "    cv=10,\n",
    "    n_jobs=4,           # Adjust CPU usage\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch on the TF-IDF training data\n",
    "dt_grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Decision Tree):\", dt_grid_search.best_params_)\n",
    "print(\"Best CV Score (Decision Tree):\", dt_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "dt_best = dt_grid_search.best_estimator_ # Get the best model\n",
    "y_pred_dt = dt_best.predict(X_test_tfidf) # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "dt_test_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Test Accuracy (Decision Tree):\", dt_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Store results\n",
    "results['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search.best_params_,\n",
    "    'best_cv_score': dt_grid_search.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_dt, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save Decision Tree results to a JSON file\n",
    "with open('decision_tree_results.json', 'w') as f:\n",
    "    # results['DecisionTree'] is a dictionary\n",
    "    json.dump(results['DecisionTree'], f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting your grid search\n",
    "cv_results_df = pd.DataFrame(dt_grid_search.cv_results_)\n",
    "\n",
    "# Display the first few rows to see the structure\n",
    "cv_results_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean test scores for different hyperparameter combinations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cv_results_df) + 1), cv_results_df['mean_test_score'], marker='o')\n",
    "plt.title('Cross-Validation Accuracy for Different Hyperparameters')\n",
    "plt.xlabel('Hyperparameter Combination Index')\n",
    "plt.ylabel('Mean CV Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for analysis in Excel, Google Sheets, etc.\n",
    "cv_results_df.to_csv('decision_tree_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    dt_classifier, X, y, param_name=\"max_depth\", param_range=param_range,\n",
    "    scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range, train_mean, label=\"Training Score\", color='blue', marker='o')\n",
    "plt.plot(param_range, test_mean, label=\"Validation Score\", color='green', linestyle='--', marker='x')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "plt.title(\"Validation Curve for max_depth\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = dt_grid_search.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 (Linear SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 2: SVM (LinearSVC)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 2: LinearSVC\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "svm_classifier = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "svm_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (SVM):\", svm_grid_search.best_params_)\n",
    "print(\"Best CV Score (SVM):\", svm_grid_search.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best = svm_grid_search.best_estimator_\n",
    "y_pred_svm = svm_best.predict(X_test_tfidf)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "svm_test_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Test Accuracy (SVM):\", svm_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Store results\n",
    "results['SVM'] = {\n",
    "    'best_params': svm_grid_search.best_params_,\n",
    "    'best_cv_score': svm_grid_search.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After svm_grid_search.fit(X_train_tfidf, y_train)\n",
    "svm_cv_results_df = pd.DataFrame(svm_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "svm_fold_columns = [col for col in svm_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "svm_cv_fold_scores = svm_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + svm_fold_columns]\n",
    "print(\"\\n--- SVM: Cross-Validation Scores for Each Fold ---\")\n",
    "print(svm_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "svm_cv_results_df.to_csv('svm_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 3: RANDOM FOREST\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 3: Random Forest\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "# Add if model overfits too much\n",
    "# 'min_samples_split': [2, 5],\n",
    "# 'min_samples_leaf': [1, 2],\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']   # Usually beneficial for text\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "rf_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (Random Forest):\", rf_grid_search.best_params_)\n",
    "print(\"Best CV Score (Random Forest):\", rf_grid_search.best_score_)\n",
    "\n",
    "# Predict\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test_tfidf)\n",
    "\n",
    "rf_test_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Test Accuracy (Random Forest):\", rf_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Store results\n",
    "results['RandomForest'] = {\n",
    "    'best_params': rf_grid_search.best_params_,\n",
    "    'best_cv_score': rf_grid_search.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After rf_grid_search.fit(X_train_tfidf, y_train)\n",
    "rf_cv_results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "rf_fold_columns = [col for col in rf_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "rf_cv_fold_scores = rf_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + rf_fold_columns]\n",
    "print(\"\\n--- Random Forest: Cross-Validation Scores for Each Fold ---\")\n",
    "print(rf_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "rf_cv_results_df.to_csv('random_forest_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 4: kNN\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 4: kNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "knn_grid_search = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "knn_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (kNN):\", knn_grid_search.best_params_)\n",
    "print(\"Best CV Score (kNN):\", knn_grid_search.best_score_)\n",
    "\n",
    "knn_best = knn_grid_search.best_estimator_\n",
    "y_pred_knn = knn_best.predict(X_test_tfidf)\n",
    "\n",
    "knn_test_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Test Accuracy (kNN):\", knn_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "results['kNN'] = {\n",
    "    'best_params': knn_grid_search.best_params_,\n",
    "    'best_cv_score': knn_grid_search.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_knn, output_dict=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After knn_grid_search.fit(X_train_tfidf, y_train)\n",
    "knn_cv_results_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "knn_fold_columns = [col for col in knn_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "knn_cv_fold_scores = knn_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + knn_fold_columns]\n",
    "print(\"\\n--- kNN: Cross-Validation Scores for Each Fold ---\")\n",
    "print(knn_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "knn_cv_results_df.to_csv('knn_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 5: NaÃ¯ve Bayes\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 5: NaÃ¯ve Bayes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "nb_grid_search = GridSearchCV(\n",
    "    estimator=nb_classifier,\n",
    "    param_grid=nb_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "nb_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (NaÃ¯ve Bayes):\", nb_grid_search.best_params_)\n",
    "print(\"Best CV Score (NaÃ¯ve Bayes):\", nb_grid_search.best_score_)\n",
    "\n",
    "nb_best = nb_grid_search.best_estimator_\n",
    "y_pred_nb = nb_best.predict(X_test_tfidf)\n",
    "\n",
    "nb_test_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Test Accuracy (NaÃ¯ve Bayes):\", nb_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "results['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search.best_params_,\n",
    "    'best_cv_score': nb_grid_search.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_nb, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After nb_grid_search.fit(X_train_tfidf, y_train)\n",
    "nb_cv_results_df = pd.DataFrame(nb_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "nb_fold_columns = [col for col in nb_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "nb_cv_fold_scores = nb_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + nb_fold_columns]\n",
    "print(\"\\n--- NaÃ¯ve Bayes: Cross-Validation Scores for Each Fold ---\")\n",
    "print(nb_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "nb_cv_results_df.to_csv('naive_bayes_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
