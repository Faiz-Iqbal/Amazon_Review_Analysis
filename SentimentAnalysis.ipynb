{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sentiment Analysis on Amazon Reviews ðŸ“Š</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The rapid growth of e-commerce, accelerated significantly during and after the COVID-19 pandemic, has reshaped consumer purchasing behaviors for both essential and non-essential goods. This shift has resulted in an overwhelming increase in online customer reviews, offering businesses a wealth of insights into customer satisfaction, product performance, and potential areas for improvement. However, the sheer volume of these reviews makes manual analysis infeasible for organizations striving to understand and act on customer sentiments effectively.\n",
    "Sentiment analysis has emerged as an essential solution, leveraging Natural Language Processing (NLP) and machine learning techniques to automatically identify and classify opinions expressed in text. This research explores the application of these techniques to analyze e-commerce reviews, aiming to uncover actionable insights at scale. By automating sentiment analysis, businesses can enhance customer experiences, personalize offerings, and make informed, data-driven decisions that align with evolving customer preferences. This study not only addresses the challenges of large-scale sentiment analysis but also highlights its transformative potential for improving business strategies in the dynamic e-commerce landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset used in this project is titled **Amazon Product Reviews** and was sourced from both Kaggle and the University of San Diegoâ€™s website. It is a publicly available dataset under the **CC0 1.0 Universal license**, which means it is free to use, share, and adapt without legal restrictions. The dataset can be accessed through [this Kaggle link](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews/data).\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset comprises the following fields:\n",
    "\n",
    "1. **Id**: A unique identifier for each review entry.\n",
    "2. **ProductId**: A unique identifier for the product being reviewed.\n",
    "3. **UserId**: A unique identifier for the user who submitted the review.\n",
    "4. **ProfileName**: The name of the user who submitted the review.\n",
    "5. **HelpfulnessNumerator**: The number of users who found the review helpful.\n",
    "6. **HelpfulnessDenominator**: The total number of users who rated the helpfulness of the review.\n",
    "7. **Score**: The rating provided by the user, typically on a scale of 1 to 5.\n",
    "8. **Time**: A timestamp representing when the review was submitted.\n",
    "9. **Summary**: A short title or summary of the review.\n",
    "10. **Text**: The full review text.\n",
    "\n",
    "### Data Preprocessing and Ethical Considerations:\n",
    "\n",
    "For this project, the **UserId** and **ProfileName** columns will be dropped from the dataset. This decision is made to ensure that no personal identifiers are used, thus maintaining ethical standards and adhering to data privacy principles. Removing these fields ensures that the dataset is ethically cleared for analysis while retaining all necessary information for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Research Questions to be Addressed\n",
    "\n",
    "- **How accurately can various machine learning models classify sentiment in e-commerce reviews?**\n",
    "- **How do different text preprocessing techniques impact the performance of sentiment classification models?**\n",
    "- **How do various feature extraction methods affect the accuracy of sentiment classification?**\n",
    "- **How do different machine learning models compare in terms of performance when classifying sentiment in e-commerce reviews?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal or command prompt to install all necessary libraries:\n",
    "\n",
    "```bash\n",
    "pip install pandas seaborn matplotlib numpy scikit-learn nltk textblob wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the imports are mentioned here:\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "# BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data cleaning tools\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Removing special characters\n",
    "import unicodedata\n",
    "\n",
    "# Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Calculating Polarity and Subjectivity\n",
    "from textblob import TextBlob\n",
    "\n",
    "# N-grams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# for Wordscloud\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load  NLTK modules\n",
    "import nltk\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data is data where all reviews (1 star to 5 star) are taken in equal proportion to avoid overfitting or underfitting\n",
    "# 25000 Records of each star rating is taken\n",
    "balanced_data = pd.read_csv('Datasets/balanced_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Drop Unnecesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "balanced_data = balanced_data[['ProductId', 'Score', 'Text']]\n",
    "# Display the updated DataFrame\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing html tags\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and update DataFrame\n",
    "balanced_data['review'] = balanced_data['Text'].apply(strip_html)\n",
    "balanced_data = balanced_data.drop('Text', axis=1)\n",
    "\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Remove HTML tags\n",
    "# def step_2_remove_html(Text):\n",
    "#     return re.sub(r'<.*?>', '', Text)\n",
    "\n",
    "# # Apply step 2 to the 'Lowercase_Text' column\n",
    "# data['No_HTML_Text'] = data['Lowercase_Text'].apply(step_2_remove_html)\n",
    "\n",
    "# # Display the result\n",
    "# print(\"\\nStep 2: No HTML Text Sample:\")\n",
    "# data[['Text', 'No_HTML_Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. To LowerCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercaseFunction(Text):\n",
    "    return Text.lower()\n",
    "\n",
    "# Apply step 1 to the 'Text' column\n",
    "balanced_data['LowercaseReview'] = balanced_data['review'].apply(lowercaseFunction)\n",
    "\n",
    "# Display the result\n",
    "print(\"Step 1: Lowercase Text Sample:\")\n",
    "balanced_data[['Score', 'LowercaseReview']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Remove Special Characters, Numbers, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove special characters, numbers, and punctuation\n",
    "def remove_special_characters(Text):\n",
    "    return re.sub(r'[^a-z\\s]', '', Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply step 3 column\n",
    "balanced_data['Review'] = balanced_data['LowercaseReview'].apply(remove_special_characters)\n",
    "balanced_data = balanced_data.drop('LowercaseReview', axis=1)\n",
    "balanced_data = balanced_data.drop('review', axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nStep 3: Cleaned Text (No Special Characters) Sample:\")\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Create Sentiment Column (Positive, Neutral, Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify scores\n",
    "def classify_score(score):\n",
    "    if score in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    elif score in [1, 2]:\n",
    "        return 'Negative'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "balanced_data['Sentiment'] = balanced_data['Score'].apply(classify_score)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure that necessary nltk resources are downloaded\n",
    "nltk.download('punkt')  # for word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "balanced_data['Tokenized_Text'] = balanced_data['Review'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('stopwords')  # for stopwords\n",
    "\n",
    "# Get the list of stopwords in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Apply stopword removal to the tokenized text\n",
    "balanced_data['Cleaned_Tokens'] = balanced_data['Tokenized_Text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')  # for WordNetLemmatizer\n",
    "nltk.download('omw-1.4')  # for WordNet lexical resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization to the cleaned tokens (without stopwords)\n",
    "balanced_data['Lemmatized_Tokens'] = balanced_data['Cleaned_Tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Display the result\n",
    "balanced_data[['Review', 'Cleaned_Tokens', 'Lemmatized_Tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (To FIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = balanced_data.copy()\n",
    "tfidf_data = tfidf_data.drop(['Score', 'Review','Tokenized_Text', 'Cleaned_Tokens'], axis=1)\n",
    "tfidf_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Join the tokens in 'Cleaned_Tokens' back into a string\n",
    "tfidf_data['Processed_Text'] = tfidf_data['Lemmatized_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the processed text data\n",
    "tfidf_matrix = vectorizer.fit_transform(tfidf_data['Processed_Text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for easier inspection\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check the first few rows of the TF-IDF matrix\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
