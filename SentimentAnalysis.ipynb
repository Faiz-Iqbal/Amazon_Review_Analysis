{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sentiment Analysis on Amazon Reviews ðŸ“Š</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The rapid growth of e-commerce, accelerated significantly during and after the COVID-19 pandemic, has reshaped consumer purchasing behaviors for both essential and non-essential goods. This shift has resulted in an overwhelming increase in online customer reviews, offering businesses a wealth of insights into customer satisfaction, product performance, and potential areas for improvement. However, the sheer volume of these reviews makes manual analysis infeasible for organizations striving to understand and act on customer sentiments effectively.\n",
    "Sentiment analysis has emerged as an essential solution, leveraging Natural Language Processing (NLP) and machine learning techniques to automatically identify and classify opinions expressed in text. This research explores the application of these techniques to analyze e-commerce reviews, aiming to uncover actionable insights at scale. By automating sentiment analysis, businesses can enhance customer experiences, personalize offerings, and make informed, data-driven decisions that align with evolving customer preferences. This study not only addresses the challenges of large-scale sentiment analysis but also highlights its transformative potential for improving business strategies in the dynamic e-commerce landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset used in this project is titled **Amazon Product Reviews** and was sourced from both Kaggle and the University of San Diegoâ€™s website. It is a publicly available dataset under the **CC0 1.0 Universal license**, which means it is free to use, share, and adapt without legal restrictions. The dataset can be accessed through [this Kaggle link](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews/data).\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset comprises the following fields:\n",
    "\n",
    "1. **Id**: A unique identifier for each review entry.\n",
    "2. **ProductId**: A unique identifier for the product being reviewed.\n",
    "3. **UserId**: A unique identifier for the user who submitted the review.\n",
    "4. **ProfileName**: The name of the user who submitted the review.\n",
    "5. **HelpfulnessNumerator**: The number of users who found the review helpful.\n",
    "6. **HelpfulnessDenominator**: The total number of users who rated the helpfulness of the review.\n",
    "7. **Score**: The rating provided by the user, typically on a scale of 1 to 5.\n",
    "8. **Time**: A timestamp representing when the review was submitted.\n",
    "9. **Summary**: A short title or summary of the review.\n",
    "10. **Text**: The full review text.\n",
    "\n",
    "### Data Preprocessing and Ethical Considerations:\n",
    "\n",
    "For this project, the **UserId** and **ProfileName** columns will be dropped from the dataset. This decision is made to ensure that no personal identifiers are used, thus maintaining ethical standards and adhering to data privacy principles. Removing these fields ensures that the dataset is ethically cleared for analysis while retaining all necessary information for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Research Questions to be Addressed\n",
    "\n",
    "- **How accurately can various machine learning models classify sentiment in e-commerce reviews?**\n",
    "- **How do different text preprocessing techniques impact the performance of sentiment classification models?**\n",
    "- **How do various feature extraction methods affect the accuracy of sentiment classification?**\n",
    "- **How do different machine learning models compare in terms of performance when classifying sentiment in e-commerce reviews?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need the following installed:\n",
    "- `pip install pandas`\n",
    "- `pip install numpy`\n",
    "- `pip install seaborn`\n",
    "- `pip install matplotlib`\n",
    "- `pip install scikit-learn`\n",
    "- `pip install nltk`\n",
    "- `pip install textblob`\n",
    "- `pip install wordcloud`\n",
    "- `pip install beautifulsoup4`\n",
    "- `pip install emoji`\n",
    "- `pip install contractions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning and Text Vectorization\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Word Clouds\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Miscellaneous\n",
    "import collections\n",
    "import emoji\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading NLTK resources\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data is data where all reviews (1 star to 5 star) are taken in equal proportion to avoid overfitting or underfitting\n",
    "# 25000 Records of each star rating is taken\n",
    "df_balancedData = pd.read_csv('Datasets/balanced_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_balancedData['Score'].value_counts().plot(\n",
    "    kind='pie', \n",
    "    autopct='%.0f%%', \n",
    "    startangle=90,  # Rotate the pie chart for better orientation\n",
    "    colors=plt.cm.Paired.colors  # Add distinct colors for each segment\n",
    ")\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Data Distribution of Scores', fontsize=16)\n",
    "plt.ylabel('')  # Remove the default y-axis label for better aesthetics\n",
    "plt.legend(title='Scores', loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine all text from the 'Review' column into a single string\n",
    "all_reviews = \" \".join(df_balancedData[\"Text\"])\n",
    "\n",
    "# Generate the Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words, \n",
    "    width=800, \n",
    "    height=800, \n",
    "    background_color='white', \n",
    "    min_font_size=10\n",
    ").generate(all_reviews)\n",
    "\n",
    "# Plot the Word Cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_data = df_balancedData.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Generate the heatmap\n",
    "heatmap = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    vmin=-1, vmax=1, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "\n",
    "# Customize the title\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 16, 'fontweight': 'bold'}, pad=20)\n",
    "\n",
    "# Rotate x and y axis labels for clarity\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Tighten the layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Unnecesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text']]\n",
    "# Display the updated DataFrame\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CleanEmojis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply emoji removal to the 'Text' column of the DataFrame\n",
    "        X['Text'] = X['Text'].apply(lambda review: emoji.demojize(review))\n",
    "        return X\n",
    "\n",
    "# Apply the transformer to the DataFrame\n",
    "cleaner = CleanEmojis()\n",
    "df_balancedData_cleaned = cleaner.transform(df_balancedData)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_balancedData_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Removing Special Characters, URLS, HTML etc. and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StringProcessing class\n",
    "class StringProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for review in X:\n",
    "            # Converting to lowercase\n",
    "            review = review.lower()\n",
    "            # Substituting ampersand signs to 'and'\n",
    "            review = re.sub(r\"&amp\", \"and\", review)\n",
    "            review = re.sub(r\"&\", \"and\", review)\n",
    "            # Removing @ mentions and hashtags\n",
    "            review = re.sub(r\"[@#][^\\s]+\", \" \", review)\n",
    "            # Removing URLs\n",
    "            review = re.sub(r\"(http\\S+)|(www\\S+)\", \"\", review)\n",
    "            # Removing HTML tags\n",
    "            review = re.sub(r\"<[^<]+?>\", \" \", review)\n",
    "            # Cleaning emojis\n",
    "            review = re.sub(r\":\", \" \", review)\n",
    "            review = review.replace(\"_\", \" \")\n",
    "            # Removing special characters\n",
    "            review = re.sub(r\"[^a-z0-9'â€™ ]\", \" \", review)\n",
    "            # Removing extra whitespace\n",
    "            review = re.sub(r\"\\s+\", \" \", review)\n",
    "            filtered_tweets.append(review)\n",
    "        return filtered_tweets\n",
    "\n",
    "# Create an instance of the StringProcessing transformer\n",
    "string_processor = StringProcessing()\n",
    "\n",
    "# Apply the transformation to the 'Text' column and store the result in a new 'Text' column\n",
    "df_balancedData.loc[:, 'Text'] = string_processor.transform(df_balancedData['Text'])\n",
    "\n",
    "# Display the updated DataFrame with the new 'Review' column\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed Text into New column 'Review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text column is not needed, so its replaced with the preprocesses 'Review' column\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text']]\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to substitute contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_short_contractions(short_contractions):\n",
    "    short_contractions_dict = {}\n",
    "    # Sort the contractions alphabetically\n",
    "    for key, value in sorted(short_contractions.items()):\n",
    "        # Remove apostrophes from the contraction keys\n",
    "        new_key = key.replace(\"'\", \"\")\n",
    "        short_contractions_dict[new_key] = value\n",
    "    return short_contractions_dict\n",
    "\n",
    "# (Maarten et al., 2013)\n",
    "short_contractions = {\n",
    "    \"i'd\": \"i would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnâ€™t\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havenâ€™t\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "short_contractions_dict = collect_short_contractions(short_contractions)\n",
    "short_contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandContractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        expanded_reviews = [contractions.fix(review) for review in X]\n",
    "        return expanded_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to handle Slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_slangs(slangs):\n",
    "    slang_dict = {}\n",
    "    for line in slangs.strip().split('\\n'):\n",
    "        key, value = line.lower().strip().split(': ', 1)  # Added limit to split on first ':'\n",
    "        slang_dict[key] = value\n",
    "\n",
    "    # Sort the dictionary by keys alphabetically\n",
    "    return dict(sorted(slang_dict.items()))\n",
    "\n",
    "# (SlickText, 2023)\n",
    "slangs = '''\n",
    "  ROFL: Rolling on the floor laughing\n",
    "  STFU: Shut up\n",
    "  ICYMI: In case you missed it\n",
    "  TL;DR: Too long, didnâ€™t read\n",
    "  TMI: Too much information\n",
    "  AFAIK: As far as I know\n",
    "  LMK: Let me know\n",
    "  NVM: Nevermind\n",
    "  FTW: For the win\n",
    "  BYOB: Bring your own beer\n",
    "  BOGO: Buy one get one\n",
    "  JK: Just kidding\n",
    "  TBH: To be honest\n",
    "  TBF: To be frank\n",
    "  RN: Right now\n",
    "  BRB: Be right back\n",
    "  BTW: By the way\n",
    "  GG: Good game\n",
    "  IRL: In real life\n",
    "  LOL: Laugh out loud\n",
    "  SMH: Shaking my head\n",
    "  NGL: Not gonna lie\n",
    "  IKR: I know right\n",
    "  TTYL: Talk to you later\n",
    "  IMO: In my opinion\n",
    "  WYD: What are you doing?\n",
    "  IDK: I donâ€™t know\n",
    "  IDC: I donâ€™t care\n",
    "  IDGAF: I donâ€™t care\n",
    "  TBA: To be announced\n",
    "  TBD: To be decided\n",
    "  AFK: Away from keyboard\n",
    "  IYKYK: If you know you know\n",
    "  B4: Before\n",
    "  FOMO: Fear of missing out\n",
    "  GTG: Got to go\n",
    "  G2G: Got to go\n",
    "  H8: Hate\n",
    "  LMAO: Laughing my ass off\n",
    "  IYKWIM: If you know what I mean\n",
    "  MYOB: Mind your own business\n",
    "  POV: Point of view\n",
    "  HBD: Happy birthday\n",
    "  WYSIWYG: What you see is what you get\n",
    "  FWIF: For what itâ€™s worth\n",
    "  TW: Trigger warning\n",
    "  EOD: End of day\n",
    "  FAQ: Frequently asked question\n",
    "  AKA: Also known as\n",
    "  ASAP: As soon as possible\n",
    "  DIY: Do it yourself\n",
    "  NP: No problem\n",
    "  U: you\n",
    "  R: are\n",
    "  PLS: please\n",
    "  WYD: what are you doing\n",
    "  N/A: Not applicable\n",
    "  K: Okay\n",
    "  WUT: what\n",
    "  FYI: For your information\n",
    "  NSFW: Not safe for work\n",
    "  WFH: Work from home\n",
    "  OMW: On my way\n",
    "  DM: Direct message\n",
    "  FB: Facebook\n",
    "  IG: Instagram\n",
    "  YT: YouTube\n",
    "  QOTD: Quote of the day\n",
    "  OOTD: Outfit of the day\n",
    "  AMA: Ask me anything\n",
    "  HMU: Hit me up\n",
    "  ILY: I love you\n",
    "  BF: Boyfriend\n",
    "  GF: Girlfriend\n",
    "  BAE: Before anyone else\n",
    "  LYSM: Love you so much\n",
    "  PDA: Public display of affection\n",
    "  XOXO: Hugs and kisses\n",
    "  LOML: Love of my life\n",
    "  THX: thanks\n",
    "  V: very\n",
    "  OMG: Oh My God\n",
    "'''\n",
    "\n",
    "slang_dict = collect_slangs(slangs)\n",
    "slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ReplaceSlangs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_reviews = []\n",
    "        for review in X:\n",
    "            # Replace slang\n",
    "            for key, value in slang_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Replace short contractions\n",
    "            for key, value in short_contractions_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            filtered_reviews.append(review)\n",
    "        return filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for tweet in X:\n",
    "          filtered_tweets.append(nltk.word_tokenize(tweet)) \n",
    "        return filtered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to remove stopwords\n",
    "**There are certain words in the built-in NLTK stop word list that if we removed, would change the sentiment of the text. Therefore, we decided to modify the original list by removing certain words like \"no\", \"against\", \"above\" etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopwords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Custom sentiment-related stopwords\n",
    "        sentiment_stopwords = ['against', 'above', 'below', 'up', 'down', 'over', 'under', 'no', 'nor', 'not', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "        \n",
    "        # NLTK's standard list of stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove sentiment-specific stopwords from the NLTK stopwords set\n",
    "        for word in sentiment_stopwords:\n",
    "            stop_words.remove(word)\n",
    "        \n",
    "        # List to hold processed texts (after stopwords removal)\n",
    "        processed_reviews = []\n",
    "        \n",
    "        # Process each review\n",
    "        for review in X:\n",
    "            # Tokenize the review (assuming it's already tokenized)\n",
    "            tokens_without_stopwords = [word for word in review if word not in stop_words]\n",
    "            \n",
    "            # Append the processed tokens for this review\n",
    "            processed_reviews.append(tokens_without_stopwords)\n",
    "        \n",
    "        return processed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Initialize the lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # List to store the lemmatized tokens for each review\n",
    "        lemmatized_reviews = []\n",
    "\n",
    "        # Define a function to convert the POS tags to WordNet tags\n",
    "        def get_wordnet_pos(pos_tag):\n",
    "            if pos_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif pos_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif pos_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif pos_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN  # Set default to noun\n",
    "        \n",
    "        # Process each review in the input data\n",
    "        for review in X:\n",
    "            # Perform POS tagging on the review\n",
    "            pos_tags = nltk.pos_tag(review)\n",
    "            \n",
    "            # Lemmatize each token based on its POS tag\n",
    "            lemmatized_tokens = [\n",
    "                lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags\n",
    "            ]\n",
    "            \n",
    "            # Append the lemmatized tokens for the current review to the list\n",
    "            lemmatized_reviews.append(lemmatized_tokens)\n",
    "        \n",
    "        # Return the lemmatized reviews (list of tokenized and lemmatized words)\n",
    "        return lemmatized_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic cleaning after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Precompile regex patterns for better performance\n",
    "        self.remove_digits = re.compile(r'\\d+')\n",
    "        self.remove_single_letters = re.compile(r'\\b\\w{1}\\b')\n",
    "        self.remove_apostrophes = re.compile(r\"'\")\n",
    "        self.remove_href = \"href\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # List to store cleaned reviews\n",
    "        cleaned_reviews = []\n",
    "\n",
    "        # Loop through each review in the input dataset\n",
    "        for review in X:\n",
    "            # Use list comprehension for efficiency in cleaning words\n",
    "            cleaned_words = [\n",
    "                self.clean_word(word)\n",
    "                for word in review\n",
    "                if word and word != self.remove_href\n",
    "            ]\n",
    "            \n",
    "            # Append the cleaned review (list of words) to the final list\n",
    "            cleaned_reviews.append(cleaned_words)\n",
    "        \n",
    "        return cleaned_reviews\n",
    "\n",
    "    def clean_word(self, word):\n",
    "        # Removing digits\n",
    "        word = self.remove_digits.sub(\"\", word)\n",
    "        \n",
    "        # Removing single letters\n",
    "        word = self.remove_single_letters.sub(\"\", word)\n",
    "        \n",
    "        # Removing apostrophes\n",
    "        word = self.remove_apostrophes.sub(\"\", word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews_frame):\n",
    "    # Define the list of preprocessing steps as tuples\n",
    "    # Each tuple contains a step name (for identification) and an instance of the corresponding transformer\n",
    "    preprocessing_steps = [\n",
    "        ('expand_contractions', ExpandContractions()),  # Step to expand contractions like \"don't\" -> \"do not\"\n",
    "        ('replace_slangs', ReplaceSlangs()),         # Step to replace slang words with their full form\n",
    "        ('tokenize', Tokenize()),                    # Step to tokenize the text (split into words)\n",
    "        ('remove_stopwords', RemoveStopwords()),     # Step to remove common stopwords\n",
    "        ('lemmatize', Lemmatize()),                  # Step to lemmatize words (reduce to root form)\n",
    "        ('final_clean', FinalClean())                # Final cleaning step (removing unwanted characters, etc.)\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline object using the preprocessing steps defined above\n",
    "    # The Pipeline applies each step sequentially to the data\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "    \n",
    "    # Apply the pipeline to the 'Text' column of the DataFrame\n",
    "    # This will preprocess the 'Text' data column by applying each step defined in the pipeline\n",
    "    preprocessed_reviews = preprocessing_pipeline.fit_transform(reviews_frame['Text'])\n",
    "    \n",
    "    # Convert the list of tokenized words into strings (space-separated)\n",
    "    # List comprehension is used to join each list of tokens into a single string for each review\n",
    "    # Each review is a list of words, so ' '.join(review) joins them back into a single string\n",
    "    preprocessed_strings = [' '.join(review) for review in preprocessed_reviews]\n",
    "\n",
    "    # Add the preprocessed reviews as a new column to the original DataFrame\n",
    "    # This new column will contain the cleaned and processed review text\n",
    "    reviews_frame['Preprocessed_Review'] = preprocessed_strings\n",
    "\n",
    "    # Return the DataFrame with the added preprocessed reviews\n",
    "    return reviews_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_reviews(df_balancedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
