{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sentiment Analysis on Amazon Reviews ðŸ“Š</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The rapid growth of e-commerce, accelerated significantly during and after the COVID-19 pandemic, has reshaped consumer purchasing behaviors for both essential and non-essential goods. This shift has resulted in an overwhelming increase in online customer reviews, offering businesses a wealth of insights into customer satisfaction, product performance, and potential areas for improvement. However, the sheer volume of these reviews makes manual analysis infeasible for organizations striving to understand and act on customer sentiments effectively.\n",
    "Sentiment analysis has emerged as an essential solution, leveraging Natural Language Processing (NLP) and machine learning techniques to automatically identify and classify opinions expressed in text. This research explores the application of these techniques to analyze e-commerce reviews, aiming to uncover actionable insights at scale. By automating sentiment analysis, businesses can enhance customer experiences, personalize offerings, and make informed, data-driven decisions that align with evolving customer preferences. This study not only addresses the challenges of large-scale sentiment analysis but also highlights its transformative potential for improving business strategies in the dynamic e-commerce landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset used in this project is titled **Amazon Product Reviews** and was sourced from both Kaggle and the University of San Diegoâ€™s website. It is a publicly available dataset under the **CC0 1.0 Universal license**, which means it is free to use, share, and adapt without legal restrictions. The dataset can be accessed through [this Kaggle link](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews/data).\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The dataset comprises the following fields:\n",
    "\n",
    "1. **Id**: A unique identifier for each review entry.\n",
    "2. **ProductId**: A unique identifier for the product being reviewed.\n",
    "3. **UserId**: A unique identifier for the user who submitted the review.\n",
    "4. **ProfileName**: The name of the user who submitted the review.\n",
    "5. **HelpfulnessNumerator**: The number of users who found the review helpful.\n",
    "6. **HelpfulnessDenominator**: The total number of users who rated the helpfulness of the review.\n",
    "7. **Score**: The rating provided by the user, typically on a scale of 1 to 5.\n",
    "8. **Time**: A timestamp representing when the review was submitted.\n",
    "9. **Summary**: A short title or summary of the review.\n",
    "10. **Text**: The full review text.\n",
    "\n",
    "### Data Preprocessing and Ethical Considerations:\n",
    "\n",
    "For this project, the **UserId** and **ProfileName** columns will be dropped from the dataset. This decision is made to ensure that no personal identifiers are used, thus maintaining ethical standards and adhering to data privacy principles. Removing these fields ensures that the dataset is ethically cleared for analysis while retaining all necessary information for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Research Questions to be Addressed\n",
    "\n",
    "- **How accurately can various machine learning models classify sentiment in e-commerce reviews?**\n",
    "- **How do different text preprocessing techniques impact the performance of sentiment classification models?**\n",
    "- **How do various feature extraction methods affect the accuracy of sentiment classification?**\n",
    "- **How do different machine learning models compare in terms of performance when classifying sentiment in e-commerce reviews?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need the following installed:\n",
    "- `pip install pandas`\n",
    "- `pip install numpy`\n",
    "- `pip install seaborn`\n",
    "- `pip install matplotlib`\n",
    "- `pip install scikit-learn`\n",
    "- `pip install nltk`\n",
    "- `pip install textblob`\n",
    "- `pip install wordcloud`\n",
    "- `pip install beautifulsoup4`\n",
    "- `pip install emoji`\n",
    "- `pip install contractions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Libraries & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local installation please uncomment the following and run this code block\n",
    "\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install nltk\n",
    "# %pip install textblob\n",
    "# %pip install wordcloud\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install emoji\n",
    "# %pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# DATA MANIPULATION & ANALYSIS\n",
    "# ===================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================================================\n",
    "# DATA VISUALIZATION\n",
    "# ===================================================\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===================================================\n",
    "# MACHINE LEARNING & MODEL SELECTION\n",
    "# ===================================================\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ===================================================\n",
    "# MACHINE LEARNING MODELS\n",
    "# ===================================================\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# ===================================================\n",
    "# FEATURE EXTRACTION & TEXT VECTORIZATION\n",
    "# ===================================================\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ===================================================\n",
    "# TEXT PREPROCESSING\n",
    "# ===================================================\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import contractions\n",
    "import collections\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ===================================================\n",
    "# SENTIMENT ANALYSIS\n",
    "# ===================================================\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ===================================================\n",
    "# WORD CLOUDS\n",
    "# ===================================================\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ===================================================\n",
    "# UTILITIES & MISCELLANEOUS\n",
    "# ===================================================\n",
    "import json  # To save results\n",
    "from tqdm import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Install NLTK Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading NLTK resources\n",
    "# Please uncomment the following line if you haven't downloaded the NLTK resources:\n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data is data where all reviews (1 star to 5 star) are taken in equal proportion to avoid overfitting or underfitting\n",
    "# 25000 Records of each star rating is taken\n",
    "df_balancedData = pd.read_csv('Datasets/balanced_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_balancedData['Score'].value_counts().plot(\n",
    "    kind='pie', \n",
    "    autopct='%.0f%%', \n",
    "    startangle=90,  # Rotate the pie chart for better orientation\n",
    "    colors=plt.cm.Paired.colors  # Add distinct colors for each segment\n",
    ")\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Data Distribution of Scores', fontsize=16)\n",
    "plt.ylabel('')  # Remove the default y-axis label for better aesthetics\n",
    "plt.legend(title='Scores', loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine all text from the 'Review' column into a single string\n",
    "all_reviews = \" \".join(df_balancedData[\"Text\"])\n",
    "\n",
    "# Generate the Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words, \n",
    "    width=800, \n",
    "    height=800, \n",
    "    background_color='white', \n",
    "    min_font_size=10\n",
    ").generate(all_reviews)\n",
    "\n",
    "# Plot the Word Cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_data = df_balancedData.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Generate the heatmap\n",
    "heatmap = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    vmin=-1, vmax=1, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "\n",
    "# Customize the title\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 16, 'fontweight': 'bold'}, pad=20)\n",
    "\n",
    "# Rotate x and y axis labels for clarity\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Tighten the layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Data Cleaning & Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Unnecesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text']]\n",
    "# Display the updated DataFrame\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Sentiment Classes from Score (3 Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#   Map 5-Star Ratings to 3 Classes\n",
    "#########################################\n",
    "def map_to_3_classes(score):\n",
    "    if score in [1, 2]:\n",
    "        return 'negative'\n",
    "    elif score == 3:\n",
    "        return 'neutral'\n",
    "    else:  # 4 or 5\n",
    "        return 'positive'\n",
    "\n",
    "df_balancedData['Sentiment'] = df_balancedData['Score'].apply(map_to_3_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CleanEmojis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply emoji removal to the 'Text' column of the DataFrame\n",
    "        tqdm.pandas(desc=\"Removing emojis\")  # Add progress bar for this step\n",
    "        X['Text'] = X['Text'].progress_apply(lambda review: emoji.demojize(review))\n",
    "        return X\n",
    "\n",
    "# Apply the transformer to the DataFrame\n",
    "cleaner = CleanEmojis()\n",
    "df_balancedData = cleaner.transform(df_balancedData)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Removing Special Characters, URLS, HTML etc. and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StringProcessing class\n",
    "class StringProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for review in X:\n",
    "            # Converting to lowercase\n",
    "            review = review.lower()\n",
    "            # Substituting ampersand signs to 'and'\n",
    "            review = re.sub(r\"&amp\", \"and\", review)\n",
    "            review = re.sub(r\"&\", \"and\", review)\n",
    "            # Removing @ mentions and hashtags\n",
    "            review = re.sub(r\"[@#][^\\s]+\", \" \", review)\n",
    "            # Removing URLs\n",
    "            review = re.sub(r\"(http\\S+)|(www\\S+)\", \"\", review)\n",
    "            # Removing HTML tags\n",
    "            review = re.sub(r\"<[^<]+?>\", \" \", review)\n",
    "            # Cleaning emojis\n",
    "            review = re.sub(r\":\", \" \", review)\n",
    "            review = review.replace(\"_\", \" \")\n",
    "            # Removing special characters\n",
    "            review = re.sub(r\"[^a-z0-9'â€™ ]\", \" \", review)\n",
    "            # Removing extra whitespace\n",
    "            review = re.sub(r\"\\s+\", \" \", review)\n",
    "            filtered_tweets.append(review)\n",
    "        return filtered_tweets\n",
    "\n",
    "# Create an instance of the StringProcessing transformer\n",
    "string_processor = StringProcessing()\n",
    "\n",
    "# Apply the transformation to the 'Text' column and store the result in a new 'Text' column\n",
    "df_balancedData.loc[:, 'Text'] = string_processor.transform(df_balancedData['Text'])\n",
    "\n",
    "# Display the updated DataFrame with the new 'Review' column\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed Text into New column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns\n",
    "df_balancedData = df_balancedData[['ProductId', 'Score', 'Text', 'Sentiment']]\n",
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to substitute contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_short_contractions(short_contractions):\n",
    "    short_contractions_dict = {}\n",
    "    # Sort the contractions alphabetically\n",
    "    for key, value in sorted(short_contractions.items()):\n",
    "        # Remove apostrophes from the contraction keys\n",
    "        new_key = key.replace(\"'\", \"\")\n",
    "        short_contractions_dict[new_key] = value\n",
    "    return short_contractions_dict\n",
    "\n",
    "# (Maarten et al., 2013)\n",
    "short_contractions = {\n",
    "    \"i'd\": \"i would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnâ€™t\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havenâ€™t\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "short_contractions_dict = collect_short_contractions(short_contractions)\n",
    "short_contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandContractions(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        expanded_reviews = []\n",
    "        # Wrap X in tqdm for a progress bar\n",
    "        for review in tqdm(X, desc=\"Expanding Contractions\"):\n",
    "            expanded_reviews.append(contractions.fix(review))\n",
    "        return expanded_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to handle Slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_slangs(slangs):\n",
    "    slang_dict = {}\n",
    "    for line in slangs.strip().split('\\n'):\n",
    "        key, value = line.lower().strip().split(': ', 1)  # Added limit to split on first ':'\n",
    "        slang_dict[key] = value\n",
    "\n",
    "    # Sort the dictionary by keys alphabetically\n",
    "    return dict(sorted(slang_dict.items()))\n",
    "\n",
    "# (SlickText, 2023)\n",
    "slangs = '''\n",
    "  ROFL: Rolling on the floor laughing\n",
    "  STFU: Shut up\n",
    "  ICYMI: In case you missed it\n",
    "  TL;DR: Too long, didnâ€™t read\n",
    "  TMI: Too much information\n",
    "  AFAIK: As far as I know\n",
    "  LMK: Let me know\n",
    "  NVM: Nevermind\n",
    "  FTW: For the win\n",
    "  BYOB: Bring your own beer\n",
    "  BOGO: Buy one get one\n",
    "  JK: Just kidding\n",
    "  TBH: To be honest\n",
    "  TBF: To be frank\n",
    "  RN: Right now\n",
    "  BRB: Be right back\n",
    "  BTW: By the way\n",
    "  GG: Good game\n",
    "  IRL: In real life\n",
    "  LOL: Laugh out loud\n",
    "  SMH: Shaking my head\n",
    "  NGL: Not gonna lie\n",
    "  IKR: I know right\n",
    "  TTYL: Talk to you later\n",
    "  IMO: In my opinion\n",
    "  WYD: What are you doing?\n",
    "  IDK: I donâ€™t know\n",
    "  IDC: I donâ€™t care\n",
    "  IDGAF: I donâ€™t care\n",
    "  TBA: To be announced\n",
    "  TBD: To be decided\n",
    "  AFK: Away from keyboard\n",
    "  IYKYK: If you know you know\n",
    "  B4: Before\n",
    "  FOMO: Fear of missing out\n",
    "  GTG: Got to go\n",
    "  G2G: Got to go\n",
    "  H8: Hate\n",
    "  LMAO: Laughing my ass off\n",
    "  IYKWIM: If you know what I mean\n",
    "  MYOB: Mind your own business\n",
    "  POV: Point of view\n",
    "  HBD: Happy birthday\n",
    "  WYSIWYG: What you see is what you get\n",
    "  FWIF: For what itâ€™s worth\n",
    "  TW: Trigger warning\n",
    "  EOD: End of day\n",
    "  FAQ: Frequently asked question\n",
    "  AKA: Also known as\n",
    "  ASAP: As soon as possible\n",
    "  DIY: Do it yourself\n",
    "  NP: No problem\n",
    "  U: you\n",
    "  R: are\n",
    "  PLS: please\n",
    "  WYD: what are you doing\n",
    "  N/A: Not applicable\n",
    "  K: Okay\n",
    "  WUT: what\n",
    "  FYI: For your information\n",
    "  NSFW: Not safe for work\n",
    "  WFH: Work from home\n",
    "  OMW: On my way\n",
    "  DM: Direct message\n",
    "  FB: Facebook\n",
    "  IG: Instagram\n",
    "  YT: YouTube\n",
    "  QOTD: Quote of the day\n",
    "  OOTD: Outfit of the day\n",
    "  AMA: Ask me anything\n",
    "  HMU: Hit me up\n",
    "  ILY: I love you\n",
    "  BF: Boyfriend\n",
    "  GF: Girlfriend\n",
    "  BAE: Before anyone else\n",
    "  LYSM: Love you so much\n",
    "  PDA: Public display of affection\n",
    "  XOXO: Hugs and kisses\n",
    "  LOML: Love of my life\n",
    "  THX: thanks\n",
    "  V: very\n",
    "  OMG: Oh My God\n",
    "'''\n",
    "\n",
    "slang_dict = collect_slangs(slangs)\n",
    "slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ReplaceSlangs(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_reviews = []\n",
    "        for review in tqdm(X, desc=\"Replacing Slangs\"):\n",
    "            # Replace slang\n",
    "            for key, value in slang_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Replace short contractions\n",
    "            for key, value in short_contractions_dict.items():\n",
    "                review = re.sub(r'\\b{}\\b'.format(re.escape(key)), value, review, flags=re.IGNORECASE)\n",
    "            \n",
    "            filtered_reviews.append(review)\n",
    "        return filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        filtered_tweets = []\n",
    "        for tweet in tqdm(X, desc=\"Tokenizing\"):\n",
    "            filtered_tweets.append(nltk.word_tokenize(tweet))\n",
    "        return filtered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to remove stopwords\n",
    "**There are certain words in the built-in NLTK stop word list that if we removed, would change the sentiment of the text. Therefore, we decided to modify the original list by removing certain words like \"no\", \"against\", \"above\" etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopwords(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        sentiment_stopwords = [\n",
    "            'against', 'above', 'below', 'up', 'down', 'over', 'under', 'no', 'nor', 'not',\n",
    "            \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "            'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "            'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "            'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "        ]\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for word in sentiment_stopwords:\n",
    "            stop_words.remove(word)\n",
    "        \n",
    "        processed_reviews = []\n",
    "        for review in tqdm(X, desc=\"Removing Stopwords\"):\n",
    "            tokens_without_stopwords = [word for word in review if word not in stop_words]\n",
    "            processed_reviews.append(tokens_without_stopwords)\n",
    "        \n",
    "        return processed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_reviews = []\n",
    "\n",
    "        def get_wordnet_pos(pos_tag):\n",
    "            if pos_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif pos_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif pos_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif pos_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN\n",
    "        \n",
    "        # Wrap X in tqdm\n",
    "        for review in tqdm(X, desc=\"Lemmatizing\"):\n",
    "            pos_tags = nltk.pos_tag(review)\n",
    "            lemmatized_tokens = [\n",
    "                lemmatizer.lemmatize(token, get_wordnet_pos(tag)) \n",
    "                for token, tag in pos_tags\n",
    "            ]\n",
    "            lemmatized_reviews.append(lemmatized_tokens)\n",
    "        \n",
    "        return lemmatized_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic cleaning after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.remove_digits = re.compile(r'\\d+')\n",
    "        self.remove_single_letters = re.compile(r'\\b\\w{1}\\b')\n",
    "        self.remove_apostrophes = re.compile(r\"'\")\n",
    "        self.remove_href = \"href\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        cleaned_reviews = []\n",
    "        \n",
    "        for review in tqdm(X, desc=\"Final Cleaning\"):\n",
    "            cleaned_words = [\n",
    "                self.clean_word(word)\n",
    "                for word in review\n",
    "                if word and word != self.remove_href\n",
    "            ]\n",
    "            cleaned_reviews.append(cleaned_words)\n",
    "        \n",
    "        return cleaned_reviews\n",
    "\n",
    "    def clean_word(self, word):\n",
    "        # Removing digits\n",
    "        word = self.remove_digits.sub(\"\", word)\n",
    "        # Removing single letters\n",
    "        word = self.remove_single_letters.sub(\"\", word)\n",
    "        # Removing apostrophes\n",
    "        word = self.remove_apostrophes.sub(\"\", word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews_frame):\n",
    "    # Define the list of preprocessing steps as tuples\n",
    "    # Each tuple contains a step name (for identification) and an instance of the corresponding transformer\n",
    "    preprocessing_steps = [\n",
    "        ('expand_contractions', ExpandContractions()),  # Step to expand contractions like \"don't\" -> \"do not\"\n",
    "        ('replace_slangs', ReplaceSlangs()),         # Step to replace slang words with their full form\n",
    "        ('tokenize', Tokenize()),                    # Step to tokenize the text (split into words)\n",
    "        ('remove_stopwords', RemoveStopwords()),     # Step to remove common stopwords\n",
    "        ('lemmatize', Lemmatize()),                  # Step to lemmatize words (reduce to root form)\n",
    "        ('final_clean', FinalClean())                # Final cleaning step (removing unwanted characters, etc.)\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline object using the preprocessing steps defined above\n",
    "    # The Pipeline applies each step sequentially to the data\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "    \n",
    "    # Apply the pipeline to the 'Text' column of the DataFrame\n",
    "    # This will preprocess the 'Text' data column by applying each step defined in the pipeline\n",
    "    preprocessed_reviews = preprocessing_pipeline.fit_transform(reviews_frame['Text'])\n",
    "    \n",
    "    # Convert the list of tokenized words into strings (space-separated)\n",
    "    # List comprehension is used to join each list of tokens into a single string for each review\n",
    "    # Each review is a list of words, so ' '.join(review) joins them back into a single string\n",
    "    preprocessed_strings = [' '.join(review) for review in preprocessed_reviews]\n",
    "\n",
    "    # Add the preprocessed reviews as a new column to the original DataFrame\n",
    "    # This new column will contain the cleaned and processed review text\n",
    "    reviews_frame['Preprocessed_Review'] = preprocessed_strings\n",
    "\n",
    "    # Return the DataFrame with the added preprocessed reviews\n",
    "    return reviews_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_reviews(df_balancedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balancedData.to_csv('Datasets/preprocessed_reviews.csv', index=False) # Save the preprocessed data to a new CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ¢ Classification Tri-Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_csv('Datasets/preprocessed_reviews.csv')\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. View the raw counts\n",
    "print(\"Class Distribution (Counts):\")\n",
    "print(df_preprocessed['Sentiment'].value_counts())\n",
    "\n",
    "# 2. View the normalized distribution (percentages)\n",
    "print(\"\\nClass Distribution (Percentages):\")\n",
    "print(df_preprocessed['Sentiment'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# 3. Visualize with a Seaborn countplot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Sentiment', data=df_preprocessed, order=['negative','neutral','positive'])  # or remove 'order' if you want auto-sorting\n",
    "plt.title(\"Distribution of Sentiment Classes\")\n",
    "plt.xlabel(\"Sentiment Class\")\n",
    "plt.ylabel(\"Count of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Features (Preprocessed Text) and Target (Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features (Preprocessed Text) and Target (Sentiment)\n",
    "X = df_preprocessed['Preprocessed_Review']\n",
    "y = df_preprocessed['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,      # 70 train 30 test\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 1:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Here, we run each model separatelyâ€”each with its own hyperparameter tuning and 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method : TFIDF Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Transformation\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Can be varied\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create a dictionary to store the final results from each model (optional)\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 1: DECISION TREE\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 1: Decision Tree (Tri-Class)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=dt_param_grid,\n",
    "    scoring='accuracy',  \n",
    "    cv=10,\n",
    "    n_jobs=4,           # Adjust CPU usage\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch on the TF-IDF training data\n",
    "dt_grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Decision Tree):\", dt_grid_search.best_params_)\n",
    "print(\"Best CV Score (Decision Tree):\", dt_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "dt_best = dt_grid_search.best_estimator_ # Get the best model\n",
    "y_pred_dt = dt_best.predict(X_test_tfidf) # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "dt_test_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Test Accuracy (Decision Tree):\", dt_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Store results\n",
    "results['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search.best_params_,\n",
    "    'best_cv_score': dt_grid_search.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_dt, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Decision Tree results to a JSON file\n",
    "with open('decision_tree_results.json', 'w') as f:\n",
    "    # results['DecisionTree'] is a dictionary\n",
    "    json.dump(results['DecisionTree'], f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting your grid search\n",
    "cv_results_df = pd.DataFrame(dt_grid_search.cv_results_)\n",
    "\n",
    "# Display the first few rows to see the structure\n",
    "cv_results_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean test scores for different hyperparameter combinations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cv_results_df) + 1), cv_results_df['mean_test_score'], marker='o')\n",
    "plt.title('Cross-Validation Accuracy for Different Hyperparameters')\n",
    "plt.xlabel('Hyperparameter Combination Index')\n",
    "plt.ylabel('Mean CV Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for analysis in Excel, Google Sheets, etc.\n",
    "cv_results_df.to_csv('Results_3Classes/TFIDF_Models/tfidf_decisionTree_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    dt_classifier, X_train_tfidf, y_train, param_name=\"max_depth\", param_range=param_range,\n",
    "    scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range, train_mean, label=\"Training Score\", color='blue', marker='o')\n",
    "plt.plot(param_range, test_mean, label=\"Validation Score\", color='green', linestyle='--', marker='x')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "plt.title(\"Validation Curve for max_depth\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = dt_grid_search.predict(X_test)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 (Linear SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 2: SVM (LinearSVC)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 2: LinearSVC\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "svm_classifier = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "svm_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (SVM):\", svm_grid_search.best_params_)\n",
    "print(\"Best CV Score (SVM):\", svm_grid_search.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best = svm_grid_search.best_estimator_\n",
    "y_pred_svm = svm_best.predict(X_test_tfidf)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "svm_test_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Test Accuracy (SVM):\", svm_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Store results\n",
    "results['SVM'] = {\n",
    "    'best_params': svm_grid_search.best_params_,\n",
    "    'best_cv_score': svm_grid_search.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After svm_grid_search.fit(X_train_tfidf, y_train)\n",
    "svm_cv_results_df = pd.DataFrame(svm_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "svm_fold_columns = [col for col in svm_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "svm_cv_fold_scores = svm_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + svm_fold_columns]\n",
    "print(\"\\n--- SVM: Cross-Validation Scores for Each Fold ---\")\n",
    "print(svm_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "svm_cv_results_df.to_csv('Results_3Classes/TFIDF_Models/tfidf_svm_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 3: RANDOM FOREST\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 3: Random Forest\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "# Add if model overfits too much\n",
    "# 'min_samples_split': [2, 5],\n",
    "# 'min_samples_leaf': [1, 2],\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']   # Usually beneficial for text\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "rf_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (Random Forest):\", rf_grid_search.best_params_)\n",
    "print(\"Best CV Score (Random Forest):\", rf_grid_search.best_score_)\n",
    "\n",
    "# Predict\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test_tfidf)\n",
    "\n",
    "rf_test_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Test Accuracy (Random Forest):\", rf_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Store results\n",
    "results['RandomForest'] = {\n",
    "    'best_params': rf_grid_search.best_params_,\n",
    "    'best_cv_score': rf_grid_search.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After rf_grid_search.fit(X_train_tfidf, y_train)\n",
    "rf_cv_results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "rf_fold_columns = [col for col in rf_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "rf_cv_fold_scores = rf_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + rf_fold_columns]\n",
    "print(\"\\n--- Random Forest: Cross-Validation Scores for Each Fold ---\")\n",
    "print(rf_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "rf_cv_results_df.to_csv('Results_3Classes/TFIDF_Models/tfidf_RandomForest_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 4: kNN\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 4: kNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "knn_grid_search = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "knn_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (kNN):\", knn_grid_search.best_params_)\n",
    "print(\"Best CV Score (kNN):\", knn_grid_search.best_score_)\n",
    "\n",
    "knn_best = knn_grid_search.best_estimator_\n",
    "y_pred_knn = knn_best.predict(X_test_tfidf)\n",
    "\n",
    "knn_test_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Test Accuracy (kNN):\", knn_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "results['kNN'] = {\n",
    "    'best_params': knn_grid_search.best_params_,\n",
    "    'best_cv_score': knn_grid_search.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_knn, output_dict=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After knn_grid_search.fit(X_train_tfidf, y_train)\n",
    "knn_cv_results_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "knn_fold_columns = [col for col in knn_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "knn_cv_fold_scores = knn_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + knn_fold_columns]\n",
    "print(\"\\n--- kNN: Cross-Validation Scores for Each Fold ---\")\n",
    "print(knn_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "knn_cv_results_df.to_csv('Results_3Classes/TFIDF_Models/tfidf_knn_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 5: NaÃ¯ve Bayes\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 5: NaÃ¯ve Bayes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "nb_grid_search = GridSearchCV(\n",
    "    estimator=nb_classifier,\n",
    "    param_grid=nb_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "nb_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"\\nBest Params (NaÃ¯ve Bayes):\", nb_grid_search.best_params_)\n",
    "print(\"Best CV Score (NaÃ¯ve Bayes):\", nb_grid_search.best_score_)\n",
    "\n",
    "nb_best = nb_grid_search.best_estimator_\n",
    "y_pred_nb = nb_best.predict(X_test_tfidf)\n",
    "\n",
    "nb_test_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Test Accuracy (NaÃ¯ve Bayes):\", nb_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "results['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search.best_params_,\n",
    "    'best_cv_score': nb_grid_search.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy,\n",
    "    'classification_report': classification_report(y_test, y_pred_nb, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After nb_grid_search.fit(X_train_tfidf, y_train)\n",
    "nb_cv_results_df = pd.DataFrame(nb_grid_search.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "nb_fold_columns = [col for col in nb_cv_results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "nb_cv_fold_scores = nb_cv_results_df[['params', 'mean_test_score', 'std_test_score'] + nb_fold_columns]\n",
    "print(\"\\n--- NaÃ¯ve Bayes: Cross-Validation Scores for Each Fold ---\")\n",
    "print(nb_cv_fold_scores)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "nb_cv_results_df.to_csv('Results_3Classes/TFIDF_Models/tfidf_NaiveBayes_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Trigram TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "# Dictionary of your best models for Tri-Class classification (TF-IDF)\n",
    "models_triclass = {\n",
    "    \"DecisionTree\": dt_best,       # from your dt_grid_search\n",
    "    \"SVM\": svm_best,               # from your svm_grid_search\n",
    "    \"RandomForest\": rf_best,       # from your rf_grid_search\n",
    "    \"kNN\": knn_best,               # from your knn_grid_search\n",
    "    \"NaiveBayes\": nb_best          # from your nb_grid_search\n",
    "}\n",
    " \n",
    "# Generate and display the confusion matrix for each model\n",
    "for model_name, model in models_triclass.items():\n",
    "    y_pred = model.predict(X_test_tfidf)         # X_test_tfidf from your Tri-Class TF-IDF setup\n",
    "    cm = confusion_matrix(y_test, y_pred)        # y_test is your Tri-Class test labels\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, \n",
    "        display_labels=model.classes_            # Ensures the labels match your modelâ€™s classes\n",
    "    )\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Tri-Class, TF-IDF)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 2:** N-Gram (Tri-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method: N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NLP Method 2: N-Gram Approach\n",
    "# ================================\n",
    "print(\"\\n=== NLP Method 2: N-Gram Approach (Unigrams, Bigrams, Trigrams) ===\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for (1,3)\n",
    "ngram_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1,3),  # (1,3) => unigrams + bigrams + trigrams\n",
    "    max_features=10000,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "# Fit on the same X_train, transform X_train and X_test\n",
    "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "# (Optional) create a dictionary to store the final results for N-Gram method\n",
    "results_ngram = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 1: Decision Tree\n",
    "# -------------------------------\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_param_grid = {\n",
    "    'max_depth': [10, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "dt_grid_search_ngram = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=dt_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Decision Tree\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dt_grid_search_ngram.fit(X_train_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Params (N-Gram, Decision Tree):\", dt_grid_search_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Decision Tree):\", dt_grid_search_ngram.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_ngram = dt_grid_search_ngram.best_estimator_\n",
    "y_pred_dt_ngram = dt_best_ngram.predict(X_test_ngram)\n",
    "\n",
    "dt_test_accuracy_ngram = accuracy_score(y_test, y_pred_dt_ngram)\n",
    "print(\"Test Accuracy (N-Gram, Decision Tree):\", dt_test_accuracy_ngram)\n",
    "print(classification_report(y_test, y_pred_dt_ngram))\n",
    "\n",
    "results_ngram['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_ngram.best_params_,\n",
    "    'best_cv_score': dt_grid_search_ngram.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_ngram,\n",
    "    'classification_report': classification_report(y_test, y_pred_dt_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV in \"Results/NGram_Models\"\n",
    "dt_cv_results_ngram = pd.DataFrame(dt_grid_search_ngram.cv_results_)\n",
    "dt_cv_results_ngram.to_csv('Results_3Classes/NGram_Models/ngram_decision_tree_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 2: Linear SVM\n",
    "# -------------------------------\n",
    "svm_classifier = LinearSVC(max_iter=10000)\n",
    "svm_param_grid = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "svm_grid_search_ngram = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Linear SVM\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "svm_grid_search_ngram.fit(X_train_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Params (N-Gram, SVM):\", svm_grid_search_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, SVM):\", svm_grid_search_ngram.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_ngram = svm_grid_search_ngram.best_estimator_\n",
    "y_pred_svm_ngram = svm_best_ngram.predict(X_test_ngram)\n",
    "\n",
    "svm_test_accuracy_ngram = accuracy_score(y_test, y_pred_svm_ngram)\n",
    "print(\"Test Accuracy (N-Gram, SVM):\", svm_test_accuracy_ngram)\n",
    "print(classification_report(y_test, y_pred_svm_ngram))\n",
    "\n",
    "results_ngram['SVM'] = {\n",
    "    'best_params': svm_grid_search_ngram.best_params_,\n",
    "    'best_cv_score': svm_grid_search_ngram.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_ngram,\n",
    "    'classification_report': classification_report(y_test, y_pred_svm_ngram, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_results_ngram = pd.DataFrame(svm_grid_search_ngram.cv_results_)\n",
    "svm_cv_results_ngram.to_csv('Results_3Classes/NGram_Models/ngram_svm_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 3: Random Forest\n",
    "# -------------------------------\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid_search_ngram = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Random Forest\")\n",
    "print(\"=\"*50)\n",
    "rf_grid_search_ngram.fit(X_train_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Params (N-Gram, Random Forest):\", rf_grid_search_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Random Forest):\", rf_grid_search_ngram.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_ngram = rf_grid_search_ngram.best_estimator_\n",
    "y_pred_rf_ngram = rf_best_ngram.predict(X_test_ngram)\n",
    "\n",
    "rf_test_accuracy_ngram = accuracy_score(y_test, y_pred_rf_ngram)\n",
    "print(\"Test Accuracy (N-Gram, Random Forest):\", rf_test_accuracy_ngram)\n",
    "print(classification_report(y_test, y_pred_rf_ngram))\n",
    "\n",
    "results_ngram['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_ngram.best_params_,\n",
    "    'best_cv_score': rf_grid_search_ngram.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_ngram,\n",
    "    'classification_report': classification_report(y_test, y_pred_rf_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "rf_cv_results_ngram = pd.DataFrame(rf_grid_search_ngram.cv_results_)\n",
    "rf_cv_results_ngram.to_csv('Results_3Classes/NGram_Models/ngram_random_forest_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4: kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 4: kNN\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "knn_grid_search_ngram = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: kNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "knn_grid_search_ngram.fit(X_train_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Params (N-Gram, kNN):\", knn_grid_search_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, kNN):\", knn_grid_search_ngram.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_ngram = knn_grid_search_ngram.best_estimator_\n",
    "y_pred_knn_ngram = knn_best_ngram.predict(X_test_ngram)\n",
    "\n",
    "knn_test_accuracy_ngram = accuracy_score(y_test, y_pred_knn_ngram)\n",
    "print(\"Test Accuracy (N-Gram, kNN):\", knn_test_accuracy_ngram)\n",
    "print(classification_report(y_test, y_pred_knn_ngram))\n",
    "\n",
    "results_ngram['kNN'] = {\n",
    "    'best_params': knn_grid_search_ngram.best_params_,\n",
    "    'best_cv_score': knn_grid_search_ngram.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_ngram,\n",
    "    'classification_report': classification_report(y_test, y_pred_knn_ngram, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_results_ngram = pd.DataFrame(knn_grid_search_ngram.cv_results_)\n",
    "knn_cv_results_ngram.to_csv('Results_3Classes/NGram_Models/ngram_knn_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 5: NaÃ¯ve Bayes\n",
    "# -------------------------------\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "nb_grid_search_ngram = GridSearchCV(\n",
    "    estimator=nb_classifier,\n",
    "    param_grid=nb_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: NaÃ¯ve Bayes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nb_grid_search_ngram.fit(X_train_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Params (N-Gram, NaÃ¯ve Bayes):\", nb_grid_search_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, NaÃ¯ve Bayes):\", nb_grid_search_ngram.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_best_ngram = nb_grid_search_ngram.best_estimator_\n",
    "y_pred_nb_ngram = nb_best_ngram.predict(X_test_ngram)\n",
    "\n",
    "nb_test_accuracy_ngram = accuracy_score(y_test, y_pred_nb_ngram)\n",
    "print(\"Test Accuracy (N-Gram, NaÃ¯ve Bayes):\", nb_test_accuracy_ngram)\n",
    "print(classification_report(y_test, y_pred_nb_ngram))\n",
    "\n",
    "results_ngram['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_ngram.best_params_,\n",
    "    'best_cv_score': nb_grid_search_ngram.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_ngram,\n",
    "    'classification_report': classification_report(y_test, y_pred_nb_ngram, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cv_results_ngram = pd.DataFrame(nb_grid_search_ngram.cv_results_)\n",
    "nb_cv_results_ngram.to_csv('Results_3Classes/NGram_Models/ngram_naive_bayes_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY FOR N-GRAM\n",
    "# ================================\n",
    "print(\"\\n=== Final Results (N-Gram) ===\")\n",
    "pd.DataFrame(results_ngram).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Trigram Ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of your best N-Gram models for Tri-Class classification\n",
    "models_triclass_ngram = {\n",
    "    \"DecisionTree\": dt_best_ngram,       # from your dt_grid_search_ngram\n",
    "    \"SVM\": svm_best_ngram,               # from your svm_grid_search_ngram\n",
    "    \"RandomForest\": rf_best_ngram,       # from your rf_grid_search_ngram\n",
    "    \"kNN\": knn_best_ngram,               # from your knn_grid_search_ngram\n",
    "    \"NaiveBayes\": nb_best_ngram          # from your nb_grid_search_ngram\n",
    "}\n",
    " \n",
    "# Generate and display the confusion matrix for each model\n",
    "for model_name, model in models_triclass_ngram.items():\n",
    "    # X_test_ngram: your N-Gram test features for the Tri-Class setup\n",
    "    # y_test:       your Tri-Class test labels (negative, neutral, positive)\n",
    "    y_pred = model.predict(X_test_ngram)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Tri-Class, N-Gram)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”µ Classification Dual-Class (General)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy of the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_2C = pd.read_csv('Datasets/preprocessed_reviews.csv')\n",
    "df_preprocessed_2C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_2C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 3 Star Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all neutral sentiment reviews to create a dual-class dataset\n",
    "df_preprocessed_2C = df_preprocessed_2C[df_preprocessed_2C['Sentiment'] != 'neutral'].copy()\n",
    "\n",
    "# Verify the distribution of sentiment classes\n",
    "print(\"Updated Class Distribution (Dual-Class General):\")\n",
    "print(df_preprocessed_2C['Sentiment'].value_counts())\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "df_preprocessed_2C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Sentiment', data=df_preprocessed_2C, order=['negative','neutral','positive'])  # or remove 'order' if you want auto-sorting\n",
    "plt.title(\"Distribution of Sentiment Classes\")\n",
    "plt.xlabel(\"Sentiment Class\")\n",
    "plt.ylabel(\"Count of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_2C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Features (Preprocessed Text) and Target (Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features (Preprocessed Text) and Target (Sentiment) for Dual-Class (General)\n",
    "X_2C = df_preprocessed_2C['Preprocessed_Review']\n",
    "y_2C = df_preprocessed_2C['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_2C.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split for Dual-Class (General)\n",
    "X_train_2C, X_test_2C, y_train_2C, y_test_2C = train_test_split(\n",
    "    X_2C, \n",
    "    y_2C, \n",
    "    test_size=0.3,  # Keeping 70-30 split\n",
    "    random_state=42, \n",
    "    stratify=y_2C  # Ensures class balance in train-test split\n",
    ")\n",
    "\n",
    "# Confirm the new distribution\n",
    "print(y_train_2C.value_counts())\n",
    "print(y_test_2C.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 1:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Here, we run each model separatelyâ€”each with its own hyperparameter tuning and 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method : TFIDF Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Transformation for Dual-Class (General)\n",
    "tfidf_vectorizer_2C = TfidfVectorizer(\n",
    "    max_features=10000,  # Keeping same as Tri-Class\n",
    "    ngram_range=(1,2),  # Unigram and Bigram\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf_2C = tfidf_vectorizer_2C.fit_transform(X_train_2C)\n",
    "X_test_tfidf_2C = tfidf_vectorizer_2C.transform(X_test_2C)\n",
    "\n",
    "# Create a dictionary to store the final results from each model (optional)\n",
    "results_2C = {}\n",
    "\n",
    "# Verify shape of transformed feature set\n",
    "print(f\"TF-IDF Train Shape: {X_train_tfidf_2C.shape}\")\n",
    "print(f\"TF-IDF Test Shape: {X_test_tfidf_2C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 1: DECISION TREE (Dual-Class General)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL 1: Decision Tree (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "dt_classifier_2C = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_2C = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "dt_grid_search_2C = GridSearchCV(\n",
    "    estimator=dt_classifier_2C,\n",
    "    param_grid=dt_param_grid_2C,\n",
    "    scoring='accuracy',  \n",
    "    cv=10,\n",
    "    n_jobs=4,  # Adjust CPU usage\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch on the TF-IDF training data\n",
    "dt_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Decision Tree - Dual-Class General):\", dt_grid_search_2C.best_params_)\n",
    "print(\"Best CV Score (Decision Tree - Dual-Class General):\", dt_grid_search_2C.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "dt_best_2C = dt_grid_search_2C.best_estimator_  # Get the best model\n",
    "y_pred_dt_2C = dt_best_2C.predict(X_test_tfidf_2C)  # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "dt_test_accuracy_2C = accuracy_score(y_test_2C, y_pred_dt_2C)\n",
    "print(\"Test Accuracy (Decision Tree - Dual-Class General):\", dt_test_accuracy_2C)\n",
    "print(classification_report(y_test_2C, y_pred_dt_2C))\n",
    "\n",
    "# Store results\n",
    "results_2C['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_2C.best_params_,\n",
    "    'best_cv_score': dt_grid_search_2C.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_dt_2C, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Decision Tree results to a JSON file\n",
    "with open('Results_2C(General)/TFIDF_Models/tfidf_decisionTree_2C_results.json', 'w') as f:\n",
    "    json.dump(results_2C['DecisionTree'], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cross-validation results to DataFrame and save\n",
    "cv_results_df_2C = pd.DataFrame(dt_grid_search_2C.cv_results_)\n",
    "cv_results_df_2C.to_csv('Results_2C(General)/TFIDF_Models/tfidf_decisionTree_2C_cv_results.csv', index=False)\n",
    "\n",
    "# Plot mean test scores for different hyperparameter combinations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cv_results_df_2C) + 1), cv_results_df_2C['mean_test_score'], marker='o')\n",
    "plt.title('Cross-Validation Accuracy for Different Hyperparameters (Decision Tree - Dual-Class General)')\n",
    "plt.xlabel('Hyperparameter Combination Index')\n",
    "plt.ylabel('Mean CV Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  Validation Curve Visualization (Decision Tree - Dual-Class General)\n",
    "# -------------------------------\n",
    "param_range_2C = [5, 10, 15, 20, 25]\n",
    "\n",
    "# Compute validation curve\n",
    "train_scores_2C, test_scores_2C = validation_curve(\n",
    "    dt_classifier_2C, X_train_tfidf_2C, y_train_2C, param_name=\"max_depth\", \n",
    "    param_range=param_range_2C, scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean_2C = np.mean(train_scores_2C, axis=1)\n",
    "test_mean_2C = np.mean(test_scores_2C, axis=1)\n",
    "train_std_2C = np.std(train_scores_2C, axis=1)\n",
    "test_std_2C = np.std(test_scores_2C, axis=1)\n",
    "\n",
    "# Plot validation curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range_2C, train_mean_2C, label=\"Training Score\", color='blue', marker='o')\n",
    "plt.plot(param_range_2C, test_mean_2C, label=\"Validation Score\", color='green', linestyle='--', marker='x')\n",
    "\n",
    "# Fill areas for standard deviation\n",
    "plt.fill_between(param_range_2C, train_mean_2C - train_std_2C, train_mean_2C + train_std_2C, alpha=0.2, color='blue')\n",
    "plt.fill_between(param_range_2C, test_mean_2C - test_std_2C, test_mean_2C + test_std_2C, alpha=0.2, color='green')\n",
    "\n",
    "plt.title(\"Validation Curve for max_depth (Decision Tree - Dual-Class General)\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 (Linear SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: SVM (LinearSVC) - Dual-Class General\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL: LinearSVC (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "svm_classifier_2C = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid_2C = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "svm_grid_search_2C = GridSearchCV(\n",
    "    estimator=svm_classifier_2C,\n",
    "    param_grid=svm_param_grid_2C,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "svm_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (SVM - Dual-Class General):\", svm_grid_search_2C.best_params_)\n",
    "print(\"Best CV Score (SVM - Dual-Class General):\", svm_grid_search_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best_2C = svm_grid_search_2C.best_estimator_\n",
    "y_pred_svm_2C = svm_best_2C.predict(X_test_tfidf_2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "svm_test_accuracy_2C = accuracy_score(y_test_2C, y_pred_svm_2C)\n",
    "print(\"Test Accuracy (SVM - Dual-Class General):\", svm_test_accuracy_2C)\n",
    "print(classification_report(y_test_2C, y_pred_svm_2C))\n",
    "\n",
    "# Store results\n",
    "results_2C['SVM'] = {\n",
    "    'best_params': svm_grid_search_2C.best_params_,\n",
    "    'best_cv_score': svm_grid_search_2C.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_svm_2C, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  Save and Store Results for SVM - Dual-Class General\n",
    "# -------------------------------\n",
    "\n",
    "# After svm_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)\n",
    "svm_cv_results_df_2C = pd.DataFrame(svm_grid_search_2C.cv_results_)\n",
    "\n",
    "# Print per-fold scores\n",
    "svm_fold_columns_2C = [col for col in svm_cv_results_df_2C.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "svm_cv_fold_scores_2C = svm_cv_results_df_2C[['params', 'mean_test_score', 'std_test_score'] + svm_fold_columns_2C]\n",
    "\n",
    "print(\"\\n--- SVM: Cross-Validation Scores for Each Fold (Dual-Class General) ---\")\n",
    "print(svm_cv_fold_scores_2C)\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "svm_cv_results_df_2C.to_csv('Results_2C(General)/TFIDF_Models/tfidf_dualClass_svm_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: Random Forest (Dual-Class General)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL: Random Forest (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "rf_classifier_2C = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid_2C = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search_2C = GridSearchCV(\n",
    "    estimator=rf_classifier_2C,\n",
    "    param_grid=rf_param_grid_2C,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "rf_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)\n",
    "\n",
    "print(\"\\nBest Params (Random Forest - Dual-Class General):\", rf_grid_search_2C.best_params_)\n",
    "print(\"Best CV Score (Random Forest - Dual-Class General):\", rf_grid_search_2C.best_score_)\n",
    "\n",
    "# Predict\n",
    "rf_best_2C = rf_grid_search_2C.best_estimator_\n",
    "y_pred_rf_2C = rf_best_2C.predict(X_test_tfidf_2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "rf_test_accuracy_2C = accuracy_score(y_test_2C, y_pred_rf_2C)\n",
    "print(\"Test Accuracy (Random Forest - Dual-Class General):\", rf_test_accuracy_2C)\n",
    "print(classification_report(y_test_2C, y_pred_rf_2C))\n",
    "\n",
    "# Store results\n",
    "results_2C['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_2C.best_params_,\n",
    "    'best_cv_score': rf_grid_search_2C.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_rf_2C, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cv_results_ to DataFrame and view per-fold scores\n",
    "rf_cv_results_df_2C = pd.DataFrame(rf_grid_search_2C.cv_results_)\n",
    "\n",
    "rf_fold_columns_2C = [col for col in rf_cv_results_df_2C.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "rf_cv_fold_scores_2C = rf_cv_results_df_2C[['params', 'mean_test_score', 'std_test_score'] + rf_fold_columns_2C]\n",
    "\n",
    "print(\"\\n--- Random Forest: Cross-Validation Scores for Each Fold (Dual-Class General) ---\")\n",
    "print(rf_cv_fold_scores_2C)\n",
    "\n",
    "# Save results to CSV using your naming convention\n",
    "rf_cv_results_df_2C.to_csv('Results_2C(General)/TFIDF_Models/tfidf_dualClass_randomForest_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: kNN (Dual-Class General)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL: kNN (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "knn_classifier_2C = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "knn_param_grid_2C = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search_2C = GridSearchCV(\n",
    "    estimator=knn_classifier_2C,\n",
    "    param_grid=knn_param_grid_2C,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "knn_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)\n",
    "\n",
    "print(\"\\nBest Params (kNN - Dual-Class General):\", knn_grid_search_2C.best_params_)\n",
    "print(\"Best CV Score (kNN - Dual-Class General):\", knn_grid_search_2C.best_score_)\n",
    "\n",
    "# Predict\n",
    "knn_best_2C = knn_grid_search_2C.best_estimator_\n",
    "y_pred_knn_2C = knn_best_2C.predict(X_test_tfidf_2C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy & Classification Report\n",
    "knn_test_accuracy_2C = accuracy_score(y_test_2C, y_pred_knn_2C)\n",
    "print(\"Test Accuracy (kNN - Dual-Class General):\", knn_test_accuracy_2C)\n",
    "print(classification_report(y_test_2C, y_pred_knn_2C))\n",
    "\n",
    "# Store results\n",
    "results_2C['kNN'] = {\n",
    "    'best_params': knn_grid_search_2C.best_params_,\n",
    "    'best_cv_score': knn_grid_search_2C.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_knn_2C, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation results\n",
    "knn_cv_results_df_2C = pd.DataFrame(knn_grid_search_2C.cv_results_)\n",
    "knn_fold_columns_2C = [col for col in knn_cv_results_df_2C.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "knn_cv_fold_scores_2C = knn_cv_results_df_2C[['params', 'mean_test_score', 'std_test_score'] + knn_fold_columns_2C]\n",
    "\n",
    "print(\"\\n--- kNN: Cross-Validation Scores for Each Fold (Dual-Class General) ---\")\n",
    "print(knn_cv_fold_scores_2C)\n",
    "\n",
    "# Save results to CSV\n",
    "knn_cv_results_df_2C.to_csv('Results_2C(General)/TFIDF_Models/tfidf_dualClass_kNN_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: NaÃ¯ve Bayes (Dual-Class General)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL: NaÃ¯ve Bayes (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "nb_classifier_2C = MultinomialNB()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "nb_param_grid_2C = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "nb_grid_search_2C = GridSearchCV(\n",
    "    estimator=nb_classifier_2C,\n",
    "    param_grid=nb_param_grid_2C,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "nb_grid_search_2C.fit(X_train_tfidf_2C, y_train_2C)\n",
    "\n",
    "print(\"\\nBest Params (NaÃ¯ve Bayes - Dual-Class General):\", nb_grid_search_2C.best_params_)\n",
    "print(\"Best CV Score (NaÃ¯ve Bayes - Dual-Class General):\", nb_grid_search_2C.best_score_)\n",
    "\n",
    "# Predict\n",
    "nb_best_2C = nb_grid_search_2C.best_estimator_\n",
    "y_pred_nb_2C = nb_best_2C.predict(X_test_tfidf_2C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & Classification Report\n",
    "nb_test_accuracy_2C = accuracy_score(y_test_2C, y_pred_nb_2C)\n",
    "print(\"Test Accuracy (NaÃ¯ve Bayes - Dual-Class General):\", nb_test_accuracy_2C)\n",
    "print(classification_report(y_test_2C, y_pred_nb_2C))\n",
    "\n",
    "# Store results\n",
    "results_2C['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_2C.best_params_,\n",
    "    'best_cv_score': nb_grid_search_2C.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_nb_2C, output_dict=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Save and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-validation results\n",
    "nb_cv_results_df_2C = pd.DataFrame(nb_grid_search_2C.cv_results_)\n",
    "nb_fold_columns_2C = [col for col in nb_cv_results_df_2C.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "nb_cv_fold_scores_2C = nb_cv_results_df_2C[['params', 'mean_test_score', 'std_test_score'] + nb_fold_columns_2C]\n",
    "\n",
    "print(\"\\n--- NaÃ¯ve Bayes: Cross-Validation Scores for Each Fold (Dual-Class General) ---\")\n",
    "print(nb_cv_fold_scores_2C)\n",
    "\n",
    "# Save results to CSV\n",
    "nb_cv_results_df_2C.to_csv('Results_2C(General)/TFIDF_Models/tfidf_dualClass_naiveBayes_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY FOR Dual-Class (General) - TF-IDF\n",
    "# ================================\n",
    "print(\"\\n=== Final Results (Dual-Class General - TF-IDF) ===\")\n",
    "\n",
    "summary_2C_df = pd.DataFrame(results_2C).T\n",
    "display(summary_2C_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class General TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Best TFâ€‘IDF models (Dual-Class General)\n",
    "models_2C_tfidf = {\n",
    "    \"DecisionTree\": dt_best_2C,       \n",
    "    \"SVM\": svm_best_2C,               \n",
    "    \"RandomForest\": rf_best_2C,       \n",
    "    \"kNN\": knn_best_2C,               \n",
    "    \"NaiveBayes\": nb_best_2C          \n",
    "}\n",
    "\n",
    "for model_name, model in models_2C_tfidf.items():\n",
    "    # X_test_tfidf_2C: your TFâ€‘IDF test data (Dual-Class General)\n",
    "    # y_test_2C:       your Dual-Class General labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_tfidf_2C)\n",
    "    cm = confusion_matrix(y_test_2C, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Dual-Class General, TF-IDF)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 2:** N-Gram (Tri-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method: N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NLP Method 2: N-Gram Approach (Dual-Class General)\n",
    "# ================================\n",
    "print(\"\\n=== NLP Method 2: N-Gram Approach (Dual-Class General) ===\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for (1,3) => unigrams, bigrams, and trigrams\n",
    "ngram_vectorizer_2C = CountVectorizer(\n",
    "    ngram_range=(1,3),  # (1,3) includes unigrams, bigrams, and trigrams\n",
    "    max_features=10000,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "# Fit on X_train_2C (the dual-class training data) and transform\n",
    "X_train_ngram_2C = ngram_vectorizer_2C.fit_transform(X_train_2C)\n",
    "X_test_ngram_2C = ngram_vectorizer_2C.transform(X_test_2C)\n",
    "\n",
    "# (Optional) create a separate dictionary to store final results from the N-Gram approach for Dual-Class\n",
    "results_ngram_2C = {}\n",
    "\n",
    "# Verify shape of transformed feature set\n",
    "print(f\"N-Gram Train Shape (2C): {X_train_ngram_2C.shape}\")\n",
    "print(f\"N-Gram Test Shape (2C): {X_test_ngram_2C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 1: Decision Tree (Dual-Class General - N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define model\n",
    "dt_classifier_2C_ngram = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_2C_ngram = {\n",
    "    'max_depth': [10, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "dt_grid_search_ngram_2C = GridSearchCV(\n",
    "    estimator=dt_classifier_2C_ngram,\n",
    "    param_grid=dt_param_grid_2C_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Decision Tree (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit the grid search on N-Gram training data\n",
    "dt_grid_search_ngram_2C.fit(X_train_ngram_2C, y_train_2C)\n",
    "\n",
    "# Display best params\n",
    "print(\"\\nBest Params (N-Gram, Decision Tree - Dual-Class General):\", dt_grid_search_ngram_2C.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Decision Tree - Dual-Class General):\", dt_grid_search_ngram_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "dt_best_ngram_2C = dt_grid_search_ngram_2C.best_estimator_\n",
    "y_pred_dt_ngram_2C = dt_best_ngram_2C.predict(X_test_ngram_2C)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "dt_test_accuracy_ngram_2C = accuracy_score(y_test_2C, y_pred_dt_ngram_2C)\n",
    "print(\"Test Accuracy (N-Gram, Decision Tree - Dual-Class General):\", dt_test_accuracy_ngram_2C)\n",
    "print(classification_report(y_test_2C, y_pred_dt_ngram_2C))\n",
    "\n",
    "# Store results\n",
    "results_ngram_2C['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_ngram_2C.best_params_,\n",
    "    'best_cv_score': dt_grid_search_ngram_2C.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_ngram_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_dt_ngram_2C, output_dict=True)\n",
    "}\n",
    "\n",
    "# Convert cross-validation results to DataFrame\n",
    "dt_cv_results_ngram_2C = pd.DataFrame(dt_grid_search_ngram_2C.cv_results_)\n",
    "\n",
    "# Save results to CSV in \"Results_2C(General)/NGram_Models\"\n",
    "dt_cv_results_ngram_2C.to_csv('Results_2C(General)/NGram_Models/ngram_dualClass_decision_tree_cv_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 2: Linear SVM (Dual-Class General - N-Gram)\n",
    "# -------------------------------\n",
    "svm_classifier_2C_ngram = LinearSVC(max_iter=10000)\n",
    "\n",
    "svm_param_grid_2C_ngram = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "svm_grid_search_ngram_2C = GridSearchCV(\n",
    "    estimator=svm_classifier_2C_ngram,\n",
    "    param_grid=svm_param_grid_2C_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Linear SVM (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit on dual-class N-Gram data\n",
    "svm_grid_search_ngram_2C.fit(X_train_ngram_2C, y_train_2C)\n",
    "\n",
    "print(\"\\nBest Params (N-Gram, SVM - Dual-Class General):\", svm_grid_search_ngram_2C.best_params_)\n",
    "print(\"Best CV Score (N-Gram, SVM - Dual-Class General):\", svm_grid_search_ngram_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best_ngram_2C = svm_grid_search_ngram_2C.best_estimator_\n",
    "y_pred_svm_ngram_2C = svm_best_ngram_2C.predict(X_test_ngram_2C)\n",
    "\n",
    "# Evaluate performance\n",
    "svm_test_accuracy_ngram_2C = accuracy_score(y_test_2C, y_pred_svm_ngram_2C)\n",
    "print(\"Test Accuracy (N-Gram, SVM - Dual-Class General):\", svm_test_accuracy_ngram_2C)\n",
    "print(classification_report(y_test_2C, y_pred_svm_ngram_2C))\n",
    "\n",
    "# Store results in n-gram dictionary\n",
    "results_ngram_2C['SVM'] = {\n",
    "    'best_params': svm_grid_search_ngram_2C.best_params_,\n",
    "    'best_cv_score': svm_grid_search_ngram_2C.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_ngram_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_svm_ngram_2C, output_dict=True)\n",
    "}\n",
    "\n",
    "# Convert cross-validation results to DataFrame\n",
    "svm_cv_results_ngram_2C = pd.DataFrame(svm_grid_search_ngram_2C.cv_results_)\n",
    "\n",
    "# Save results to CSV (dual-class general n-gram path)\n",
    "svm_cv_results_ngram_2C.to_csv('Results_2C(General)/NGram_Models/ngram_dualClass_svm_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: Random Forest (Dual-Class General - N-Gram)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Random Forest (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf_classifier_2C_ngram = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "rf_param_grid_2C_ngram = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search_ngram_2C = GridSearchCV(\n",
    "    estimator=rf_classifier_2C_ngram,\n",
    "    param_grid=rf_param_grid_2C_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit on N-Gram training data (dual-class general)\n",
    "rf_grid_search_ngram_2C.fit(X_train_ngram_2C, y_train_2C)\n",
    "\n",
    "# Print the best parameters and best CV score\n",
    "print(\"\\nBest Params (N-Gram, Random Forest - Dual-Class General):\", rf_grid_search_ngram_2C.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Random Forest - Dual-Class General):\", rf_grid_search_ngram_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "rf_best_ngram_2C = rf_grid_search_ngram_2C.best_estimator_\n",
    "y_pred_rf_ngram_2C = rf_best_ngram_2C.predict(X_test_ngram_2C)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_test_accuracy_ngram_2C = accuracy_score(y_test_2C, y_pred_rf_ngram_2C)\n",
    "print(\"Test Accuracy (N-Gram, Random Forest - Dual-Class General):\", rf_test_accuracy_ngram_2C)\n",
    "print(classification_report(y_test_2C, y_pred_rf_ngram_2C))\n",
    "\n",
    "# Store results in the n-gram dictionary for Dual-Class\n",
    "results_ngram_2C['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_ngram_2C.best_params_,\n",
    "    'best_cv_score': rf_grid_search_ngram_2C.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_ngram_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_rf_ngram_2C, output_dict=True)\n",
    "}\n",
    "\n",
    "# Convert cv_results_ to DataFrame and save\n",
    "rf_cv_results_ngram_2C = pd.DataFrame(rf_grid_search_ngram_2C.cv_results_)\n",
    "rf_cv_results_ngram_2C.to_csv('Results_2C(General)/NGram_Models/ngram_dualClass_randomForest_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: kNN (Dual-Class General - N-Gram)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: kNN (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the kNN model\n",
    "knn_classifier_2C_ngram = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "knn_param_grid_2C_ngram = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search_ngram_2C = GridSearchCV(\n",
    "    estimator=knn_classifier_2C_ngram,\n",
    "    param_grid=knn_param_grid_2C_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit on N-Gram training data\n",
    "knn_grid_search_ngram_2C.fit(X_train_ngram_2C, y_train_2C)\n",
    "\n",
    "# Print best params and best CV score\n",
    "print(\"\\nBest Params (N-Gram, kNN - Dual-Class General):\", knn_grid_search_ngram_2C.best_params_)\n",
    "print(\"Best CV Score (N-Gram, kNN - Dual-Class General):\", knn_grid_search_ngram_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "knn_best_ngram_2C = knn_grid_search_ngram_2C.best_estimator_\n",
    "y_pred_knn_ngram_2C = knn_best_ngram_2C.predict(X_test_ngram_2C)\n",
    "\n",
    "# Evaluate performance\n",
    "knn_test_accuracy_ngram_2C = accuracy_score(y_test_2C, y_pred_knn_ngram_2C)\n",
    "print(\"Test Accuracy (N-Gram, kNN - Dual-Class General):\", knn_test_accuracy_ngram_2C)\n",
    "print(classification_report(y_test_2C, y_pred_knn_ngram_2C))\n",
    "\n",
    "# Store results\n",
    "results_ngram_2C['kNN'] = {\n",
    "    'best_params': knn_grid_search_ngram_2C.best_params_,\n",
    "    'best_cv_score': knn_grid_search_ngram_2C.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_ngram_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_knn_ngram_2C, output_dict=True)\n",
    "}\n",
    "\n",
    "# Convert cv_results_ to DataFrame and save\n",
    "knn_cv_results_df_2C_ngram = pd.DataFrame(knn_grid_search_ngram_2C.cv_results_)\n",
    "knn_cv_results_df_2C_ngram.to_csv('Results_2C(General)/NGram_Models/ngram_dualClass_kNN_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION: NaÃ¯ve Bayes (Dual-Class General - N-Gram)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: NaÃ¯ve Bayes (Dual-Class General)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the Multinomial Naive Bayes model\n",
    "nb_classifier_2C_ngram = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "nb_param_grid_2C_ngram = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "nb_grid_search_ngram_2C = GridSearchCV(\n",
    "    estimator=nb_classifier_2C_ngram,\n",
    "    param_grid=nb_param_grid_2C_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit on dual-class N-Gram data\n",
    "nb_grid_search_ngram_2C.fit(X_train_ngram_2C, y_train_2C)\n",
    "\n",
    "# Print best parameters and best CV score\n",
    "print(\"\\nBest Params (N-Gram, NaÃ¯ve Bayes - Dual-Class General):\", nb_grid_search_ngram_2C.best_params_)\n",
    "print(\"Best CV Score (N-Gram, NaÃ¯ve Bayes - Dual-Class General):\", nb_grid_search_ngram_2C.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "nb_best_ngram_2C = nb_grid_search_ngram_2C.best_estimator_\n",
    "y_pred_nb_ngram_2C = nb_best_ngram_2C.predict(X_test_ngram_2C)\n",
    "\n",
    "# Evaluate performance\n",
    "nb_test_accuracy_ngram_2C = accuracy_score(y_test_2C, y_pred_nb_ngram_2C)\n",
    "print(\"Test Accuracy (N-Gram, NaÃ¯ve Bayes - Dual-Class General):\", nb_test_accuracy_ngram_2C)\n",
    "print(classification_report(y_test_2C, y_pred_nb_ngram_2C))\n",
    "\n",
    "# Store results\n",
    "results_ngram_2C['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_ngram_2C.best_params_,\n",
    "    'best_cv_score': nb_grid_search_ngram_2C.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_ngram_2C,\n",
    "    'classification_report': classification_report(y_test_2C, y_pred_nb_ngram_2C, output_dict=True)\n",
    "}\n",
    "\n",
    "# Convert cv_results_ to DataFrame\n",
    "nb_cv_results_df_2C_ngram = pd.DataFrame(nb_grid_search_ngram_2C.cv_results_)\n",
    "\n",
    "# Save results to CSV\n",
    "nb_cv_results_df_2C_ngram.to_csv('Results_2C(General)/NGram_Models/ngram_dualClass_naiveBayes_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_ngram_2C).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class General - Ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Nâ€‘Gram models (Dual-Class General)\n",
    "models_2C_ngram = {\n",
    "    \"DecisionTree\": dt_best_ngram_2C,       \n",
    "    \"SVM\": svm_best_ngram_2C,               \n",
    "    \"RandomForest\": rf_best_ngram_2C,       \n",
    "    \"kNN\": knn_best_ngram_2C,               \n",
    "    \"NaiveBayes\": nb_best_ngram_2C          \n",
    "}\n",
    "\n",
    "for model_name, model in models_2C_ngram.items():\n",
    "    # X_test_ngram_2C: your Nâ€‘Gram test data (Dual-Class General)\n",
    "    # y_test_2C:       your Dual-Class General labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_ngram_2C)\n",
    "    cm = confusion_matrix(y_test_2C, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Dual-Class General, N-Gram)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ£ Classification Dual-Class (Negative Biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy of the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negBias = pd.read_csv('Datasets/preprocessed_reviews.csv')\n",
    "df_negBias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 3 Star Reviews with Negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop the existing Sentiment column\n",
    "df_negBias.drop(columns=['Sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negBias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a function to merge 1,2,3 â†’ negative and 4,5 â†’ positive\n",
    "def map_neg_biased(score):\n",
    "    if score in [1, 2, 3]:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Recreate the Sentiment column with negative bias mapping\n",
    "df_negBias['Sentiment'] = df_negBias['Score'].apply(map_neg_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negBias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Verify the distribution\n",
    "print(\"\\nDistribution for Negative-Biased Classification:\")\n",
    "print(df_negBias['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Features (Preprocessed Text) and Target (Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Features (Preprocessed Text) and Target (Neg-Biased Sentiment)\n",
    "X_negBias = df_negBias['Preprocessed_Review']\n",
    "y_negBias = df_negBias['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_negBias, X_test_negBias, y_train_negBias, y_test_negBias = train_test_split(\n",
    "    X_negBias,\n",
    "    y_negBias,\n",
    "    test_size=0.3,      # 70 train / 30 test\n",
    "    random_state=42,\n",
    "    stratify=y_negBias\n",
    ")\n",
    "\n",
    "# 8. Print distribution in train/test sets\n",
    "print(\"\\nTraining Set Distribution:\")\n",
    "print(y_train_negBias.value_counts())\n",
    "print(\"\\nTest Set Distribution:\")\n",
    "print(y_test_negBias.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 1:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Here, we run each model separatelyâ€”each with its own hyperparameter tuning and 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method: TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# TF-IDF Feature Extraction (Negative-Biased)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n=== TF-IDF Feature Extraction for Negative-Biased Sentiment ===\")\n",
    "\n",
    "# Define the TF-IDF Vectorizer\n",
    "tfidf_vectorizer_negBias = TfidfVectorizer(\n",
    "    max_features=10000,  # Adjust as needed\n",
    "    ngram_range=(1,2),   # Unigrams & Bigrams\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf_negBias = tfidf_vectorizer_negBias.fit_transform(X_train_negBias)\n",
    "X_test_tfidf_negBias = tfidf_vectorizer_negBias.transform(X_test_negBias)\n",
    "\n",
    "# Initialize a dictionary to store model results\n",
    "results_negBias = {}\n",
    "\n",
    "print(\"\\nTF-IDF Feature Extraction Complete. Ready for Model Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# MODEL 1: Decision Tree (Negative-Biased)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Negative-Biased: Decision Tree\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the model\n",
    "dt_classifier_negBias = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_negBias = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold cross-validation\n",
    "dt_grid_search_negBias = GridSearchCV(\n",
    "    estimator=dt_classifier_negBias,\n",
    "    param_grid=dt_param_grid_negBias,\n",
    "    scoring='accuracy',  \n",
    "    cv=10,\n",
    "    n_jobs=4,           # Adjust CPU usage\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit GridSearch on the TF-IDF training data\n",
    "dt_grid_search_negBias.fit(X_train_tfidf_negBias, y_train_negBias)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Decision Tree, Negative-Biased):\", dt_grid_search_negBias.best_params_)\n",
    "print(\"Best CV Score (Decision Tree, Negative-Biased):\", dt_grid_search_negBias.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "dt_best_negBias = dt_grid_search_negBias.best_estimator_  # Get the best model\n",
    "y_pred_dt_negBias = dt_best_negBias.predict(X_test_tfidf_negBias)  # Predict on test data\n",
    "\n",
    "# Compute Accuracy & Classification Report\n",
    "dt_test_accuracy_negBias = accuracy_score(y_test_negBias, y_pred_dt_negBias)\n",
    "print(\"Test Accuracy (Decision Tree, Negative-Biased):\", dt_test_accuracy_negBias)\n",
    "print(classification_report(y_test_negBias, y_pred_dt_negBias))\n",
    "\n",
    "# Store results\n",
    "results_negBias['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_negBias.best_params_,\n",
    "    'best_cv_score': dt_grid_search_negBias.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_negBias,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_dt_negBias, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "dt_cv_results_negBias = pd.DataFrame(dt_grid_search_negBias.cv_results_)\n",
    "dt_cv_results_negBias.to_csv('Results_2C(NegBias)/TFIDF_Models/tfidf_negBias_decisionTree_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDecision Tree Model Training & Evaluation Complete for Negative-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 2: Linear SVM (Negative Bias)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: Linear SVM\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "svm_classifier_negBias = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid_negBias = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold CV\n",
    "svm_grid_search_negBias = GridSearchCV(\n",
    "    estimator=svm_classifier_negBias,\n",
    "    param_grid=svm_param_grid_negBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "svm_grid_search_negBias.fit(X_train_tfidf_negBias, y_train_negBias)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Negative-Biased, SVM):\", svm_grid_search_negBias.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, SVM):\", svm_grid_search_negBias.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best_negBias = svm_grid_search_negBias.best_estimator_\n",
    "y_pred_svm_negBias = svm_best_negBias.predict(X_test_tfidf_negBias)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "svm_test_accuracy_negBias = accuracy_score(y_test_negBias, y_pred_svm_negBias)\n",
    "print(\"Test Accuracy (Negative-Biased, SVM):\", svm_test_accuracy_negBias)\n",
    "print(classification_report(y_test_negBias, y_pred_svm_negBias))\n",
    "\n",
    "# Store results\n",
    "results_negBias['SVM'] = {\n",
    "    'best_params': svm_grid_search_negBias.best_params_,\n",
    "    'best_cv_score': svm_grid_search_negBias.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_negBias,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_svm_negBias, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "svm_cv_results_negBias = pd.DataFrame(svm_grid_search_negBias.cv_results_)\n",
    "svm_cv_results_negBias.to_csv('Results_2C(NegBias)/TFIDF_Models/tfidf_negBias_svm_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nLinear SVM Model Training & Evaluation Complete for Negative-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 3: Random Forest (Negative Bias)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: Random Forest\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "rf_classifier_negBias = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid_negBias = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']   # Usually beneficial for text\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search_negBias = GridSearchCV(\n",
    "    estimator=rf_classifier_negBias,\n",
    "    param_grid=rf_param_grid_negBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "rf_grid_search_negBias.fit(X_train_tfidf_negBias, y_train_negBias)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Negative-Biased, Random Forest):\", rf_grid_search_negBias.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, Random Forest):\", rf_grid_search_negBias.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "rf_best_negBias = rf_grid_search_negBias.best_estimator_\n",
    "y_pred_rf_negBias = rf_best_negBias.predict(X_test_tfidf_negBias)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "rf_test_accuracy_negBias = accuracy_score(y_test_negBias, y_pred_rf_negBias)\n",
    "print(\"Test Accuracy (Negative-Biased, Random Forest):\", rf_test_accuracy_negBias)\n",
    "print(classification_report(y_test_negBias, y_pred_rf_negBias))\n",
    "\n",
    "# Store results\n",
    "results_negBias['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_negBias.best_params_,\n",
    "    'best_cv_score': rf_grid_search_negBias.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_negBias,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_rf_negBias, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "rf_cv_results_negBias = pd.DataFrame(rf_grid_search_negBias.cv_results_)\n",
    "rf_cv_results_negBias.to_csv('Results_2C(NegBias)/TFIDF_Models/tfidf_negBias_randomForest_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nRandom Forest Model Training & Evaluation Complete for Negative-Biased Sentiment!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 4: kNN (Negative Bias)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: kNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "knn_classifier_negBias = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "knn_param_grid_negBias = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search_negBias = GridSearchCV(\n",
    "    estimator=knn_classifier_negBias,\n",
    "    param_grid=knn_param_grid_negBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "knn_grid_search_negBias.fit(X_train_tfidf_negBias, y_train_negBias)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Negative-Biased, kNN):\", knn_grid_search_negBias.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, kNN):\", knn_grid_search_negBias.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "knn_best_negBias = knn_grid_search_negBias.best_estimator_\n",
    "y_pred_knn_negBias = knn_best_negBias.predict(X_test_tfidf_negBias)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "knn_test_accuracy_negBias = accuracy_score(y_test_negBias, y_pred_knn_negBias)\n",
    "print(\"Test Accuracy (Negative-Biased, kNN):\", knn_test_accuracy_negBias)\n",
    "print(classification_report(y_test_negBias, y_pred_knn_negBias))\n",
    "\n",
    "# Store results\n",
    "results_negBias['kNN'] = {\n",
    "    'best_params': knn_grid_search_negBias.best_params_,\n",
    "    'best_cv_score': knn_grid_search_negBias.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_negBias,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_knn_negBias, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "knn_cv_results_negBias = pd.DataFrame(knn_grid_search_negBias.cv_results_)\n",
    "knn_cv_results_negBias.to_csv('Results_2C(NegBias)/TFIDF_Models/tfidf_negBias_knn_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nkNN Model Training & Evaluation Complete for Negative-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SECTION 5: NaÃ¯ve Bayes (Negative Bias)\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: NaÃ¯ve Bayes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define model\n",
    "nb_classifier_negBias = MultinomialNB()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "nb_param_grid_negBias = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "nb_grid_search_negBias = GridSearchCV(\n",
    "    estimator=nb_classifier_negBias,\n",
    "    param_grid=nb_param_grid_negBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "nb_grid_search_negBias.fit(X_train_tfidf_negBias, y_train_negBias)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nBest Params (Negative-Biased, NaÃ¯ve Bayes):\", nb_grid_search_negBias.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, NaÃ¯ve Bayes):\", nb_grid_search_negBias.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "nb_best_negBias = nb_grid_search_negBias.best_estimator_\n",
    "y_pred_nb_negBias = nb_best_negBias.predict(X_test_tfidf_negBias)\n",
    "\n",
    "# Accuracy & Classification Report\n",
    "nb_test_accuracy_negBias = accuracy_score(y_test_negBias, y_pred_nb_negBias)\n",
    "print(\"Test Accuracy (Negative-Biased, NaÃ¯ve Bayes):\", nb_test_accuracy_negBias)\n",
    "print(classification_report(y_test_negBias, y_pred_nb_negBias))\n",
    "\n",
    "# Store results\n",
    "results_negBias['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_negBias.best_params_,\n",
    "    'best_cv_score': nb_grid_search_negBias.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_negBias,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_nb_negBias, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "nb_cv_results_negBias = pd.DataFrame(nb_grid_search_negBias.cv_results_)\n",
    "nb_cv_results_negBias.to_csv('Results_2C(NegBias)/TFIDF_Models/tfidf_negBias_NaiveBayes_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nNaÃ¯ve Bayes Model Training & Evaluation Complete for Negative-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  SUMMARY: Negative-Biased Sentiment Classification\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: Summary of Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Convert results dictionary to DataFrame for readability\n",
    "df_results_summary_negBias = pd.DataFrame(results_negBias).T\n",
    "\n",
    "# Display summary table\n",
    "print(df_results_summary_negBias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class NEG BIAS - TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best TFâ€‘IDF models (Negative-Biased)\n",
    "models_negBias_tfidf = {\n",
    "    \"DecisionTree\": dt_best_negBias,       \n",
    "    \"SVM\": svm_best_negBias,               \n",
    "    \"RandomForest\": rf_best_negBias,       \n",
    "    \"kNN\": knn_best_negBias,               \n",
    "    \"NaiveBayes\": nb_best_negBias          \n",
    "}\n",
    "\n",
    "for model_name, model in models_negBias_tfidf.items():\n",
    "    # X_test_tfidf_negBias: your TFâ€‘IDF test data (Negative-Biased)\n",
    "    # y_test_negBias:       your Negative-Biased labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_tfidf_negBias)\n",
    "    cm = confusion_matrix(y_test_negBias, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Negative-Biased, TF-IDF)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 2:** N-Gram (Tri-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection Method: N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: N-Gram Feature Extraction\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram Feature Extraction\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for (1,3) N-Grams\n",
    "ngram_vectorizer_negBias = CountVectorizer(\n",
    "    ngram_range=(1,3),  # (1,3) => unigrams, bigrams, trigrams\n",
    "    max_features=10000,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train & test sets\n",
    "X_train_ngram_negBias = ngram_vectorizer_negBias.fit_transform(X_train_negBias)\n",
    "X_test_ngram_negBias = ngram_vectorizer_negBias.transform(X_test_negBias)\n",
    "\n",
    "# Initialize a dictionary to store model results for N-Gram approach\n",
    "results_ngram_negBias = {}\n",
    "\n",
    "print(\"\\nN-Gram Feature Extraction for Negative-Biased Sentiment Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: Decision Tree (N-Gram)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram Decision Tree\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define Decision Tree Classifier\n",
    "dt_classifier_negBias_ngram = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_negBias_ngram = {\n",
    "    'max_depth': [10, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold cross-validation\n",
    "dt_grid_search_negBias_ngram = GridSearchCV(\n",
    "    estimator=dt_classifier_negBias_ngram,\n",
    "    param_grid=dt_param_grid_negBias_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "dt_grid_search_negBias_ngram.fit(X_train_ngram_negBias, y_train_negBias)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Params (Negative-Biased, N-Gram, Decision Tree):\", dt_grid_search_negBias_ngram.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, N-Gram, Decision Tree):\", dt_grid_search_negBias_ngram.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "dt_best_negBias_ngram = dt_grid_search_negBias_ngram.best_estimator_\n",
    "y_pred_dt_negBias_ngram = dt_best_negBias_ngram.predict(X_test_ngram_negBias)\n",
    "\n",
    "# Evaluate model performance\n",
    "dt_test_accuracy_negBias_ngram = accuracy_score(y_test_negBias, y_pred_dt_negBias_ngram)\n",
    "print(\"Test Accuracy (Negative-Biased, N-Gram, Decision Tree):\", dt_test_accuracy_negBias_ngram)\n",
    "print(classification_report(y_test_negBias, y_pred_dt_negBias_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_negBias['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_negBias_ngram.best_params_,\n",
    "    'best_cv_score': dt_grid_search_negBias_ngram.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_negBias_ngram,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_dt_negBias_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "dt_cv_results_negBias_ngram = pd.DataFrame(dt_grid_search_negBias_ngram.cv_results_)\n",
    "dt_cv_results_negBias_ngram.to_csv('Results_2C(NegBias)/NGram_Models/ngram_negBias_decisionTree_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDecision Tree Model Training & Evaluation Complete for Negative-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: Linear SVM (N-Gram)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram Linear SVM\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define SVM Classifier\n",
    "svm_classifier_negBias_ngram = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid_negBias_ngram = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold cross-validation\n",
    "svm_grid_search_negBias_ngram = GridSearchCV(\n",
    "    estimator=svm_classifier_negBias_ngram,\n",
    "    param_grid=svm_param_grid_negBias_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svm_grid_search_negBias_ngram.fit(X_train_ngram_negBias, y_train_negBias)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Params (Negative-Biased, N-Gram, SVM):\", svm_grid_search_negBias_ngram.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, N-Gram, SVM):\", svm_grid_search_negBias_ngram.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "svm_best_negBias_ngram = svm_grid_search_negBias_ngram.best_estimator_\n",
    "y_pred_svm_negBias_ngram = svm_best_negBias_ngram.predict(X_test_ngram_negBias)\n",
    "\n",
    "# Evaluate model performance\n",
    "svm_test_accuracy_negBias_ngram = accuracy_score(y_test_negBias, y_pred_svm_negBias_ngram)\n",
    "print(\"Test Accuracy (Negative-Biased, N-Gram, SVM):\", svm_test_accuracy_negBias_ngram)\n",
    "print(classification_report(y_test_negBias, y_pred_svm_negBias_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_negBias['SVM'] = {\n",
    "    'best_params': svm_grid_search_negBias_ngram.best_params_,\n",
    "    'best_cv_score': svm_grid_search_negBias_ngram.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_negBias_ngram,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_svm_negBias_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "svm_cv_results_negBias_ngram = pd.DataFrame(svm_grid_search_negBias_ngram.cv_results_)\n",
    "svm_cv_results_negBias_ngram.to_csv('Results_2C(NegBias)/NGram_Models/ngram_negBias_svm_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nLinear SVM Model Training & Evaluation Complete for Negative-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: Random Forest (N-Gram)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram Random Forest\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf_classifier_negBias_ngram = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid_negBias_ngram = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search_negBias_ngram = GridSearchCV(\n",
    "    estimator=rf_classifier_negBias_ngram,\n",
    "    param_grid=rf_param_grid_negBias_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_grid_search_negBias_ngram.fit(X_train_ngram_negBias, y_train_negBias)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Params (Negative-Biased, N-Gram, Random Forest):\", rf_grid_search_negBias_ngram.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, N-Gram, Random Forest):\", rf_grid_search_negBias_ngram.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "rf_best_negBias_ngram = rf_grid_search_negBias_ngram.best_estimator_\n",
    "y_pred_rf_negBias_ngram = rf_best_negBias_ngram.predict(X_test_ngram_negBias)\n",
    "\n",
    "# Evaluate model performance\n",
    "rf_test_accuracy_negBias_ngram = accuracy_score(y_test_negBias, y_pred_rf_negBias_ngram)\n",
    "print(\"Test Accuracy (Negative-Biased, N-Gram, Random Forest):\", rf_test_accuracy_negBias_ngram)\n",
    "print(classification_report(y_test_negBias, y_pred_rf_negBias_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_negBias['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_negBias_ngram.best_params_,\n",
    "    'best_cv_score': rf_grid_search_negBias_ngram.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_negBias_ngram,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_rf_negBias_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "rf_cv_results_negBias_ngram = pd.DataFrame(rf_grid_search_negBias_ngram.cv_results_)\n",
    "rf_cv_results_negBias_ngram.to_csv('Results_2C(NegBias)/NGram_Models/ngram_negBias_randomForest_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nRandom Forest Model Training & Evaluation Complete for Negative-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4: kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: kNN (N-Gram)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram kNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define kNN Classifier\n",
    "knn_classifier_negBias_ngram = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "knn_param_grid_negBias_ngram = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search_negBias_ngram = GridSearchCV(\n",
    "    estimator=knn_classifier_negBias_ngram,\n",
    "    param_grid=knn_param_grid_negBias_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "knn_grid_search_negBias_ngram.fit(X_train_ngram_negBias, y_train_negBias)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Params (Negative-Biased, N-Gram, kNN):\", knn_grid_search_negBias_ngram.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, N-Gram, kNN):\", knn_grid_search_negBias_ngram.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "knn_best_negBias_ngram = knn_grid_search_negBias_ngram.best_estimator_\n",
    "y_pred_knn_negBias_ngram = knn_best_negBias_ngram.predict(X_test_ngram_negBias)\n",
    "\n",
    "# Evaluate model performance\n",
    "knn_test_accuracy_negBias_ngram = accuracy_score(y_test_negBias, y_pred_knn_negBias_ngram)\n",
    "print(\"Test Accuracy (Negative-Biased, N-Gram, kNN):\", knn_test_accuracy_negBias_ngram)\n",
    "print(classification_report(y_test_negBias, y_pred_knn_negBias_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_negBias['kNN'] = {\n",
    "    'best_params': knn_grid_search_negBias_ngram.best_params_,\n",
    "    'best_cv_score': knn_grid_search_negBias_ngram.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_negBias_ngram,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_knn_negBias_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "knn_cv_results_negBias_ngram = pd.DataFrame(knn_grid_search_negBias_ngram.cv_results_)\n",
    "knn_cv_results_negBias_ngram.to_csv('Results_2C(NegBias)/NGram_Models/ngram_negBias_knn_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nkNN Model Training & Evaluation Complete for Negative-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NEGATIVE-BIASED: NaÃ¯ve Bayes (N-Gram)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEGATIVE-BIASED: N-Gram NaÃ¯ve Bayes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define NaÃ¯ve Bayes Classifier\n",
    "nb_classifier_negBias_ngram = MultinomialNB()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "nb_param_grid_negBias_ngram = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "nb_grid_search_negBias_ngram = GridSearchCV(\n",
    "    estimator=nb_classifier_negBias_ngram,\n",
    "    param_grid=nb_param_grid_negBias_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nb_grid_search_negBias_ngram.fit(X_train_ngram_negBias, y_train_negBias)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Params (Negative-Biased, N-Gram, NaÃ¯ve Bayes):\", nb_grid_search_negBias_ngram.best_params_)\n",
    "print(\"Best CV Score (Negative-Biased, N-Gram, NaÃ¯ve Bayes):\", nb_grid_search_negBias_ngram.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "nb_best_negBias_ngram = nb_grid_search_negBias_ngram.best_estimator_\n",
    "y_pred_nb_negBias_ngram = nb_best_negBias_ngram.predict(X_test_ngram_negBias)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_test_accuracy_negBias_ngram = accuracy_score(y_test_negBias, y_pred_nb_negBias_ngram)\n",
    "print(\"Test Accuracy (Negative-Biased, N-Gram, NaÃ¯ve Bayes):\", nb_test_accuracy_negBias_ngram)\n",
    "print(classification_report(y_test_negBias, y_pred_nb_negBias_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_negBias['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_negBias_ngram.best_params_,\n",
    "    'best_cv_score': nb_grid_search_negBias_ngram.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_negBias_ngram,\n",
    "    'classification_report': classification_report(y_test_negBias, y_pred_nb_negBias_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "nb_cv_results_negBias_ngram = pd.DataFrame(nb_grid_search_negBias_ngram.cv_results_)\n",
    "nb_cv_results_negBias_ngram.to_csv('Results_2C(NegBias)/NGram_Models/ngram_negBias_naiveBayes_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nNaÃ¯ve Bayes Model Training & Evaluation Complete for Negative-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY FOR NEGATIVE-BIASED (N-GRAM)\n",
    "# ================================\n",
    "print(\"\\n=== Final Results (Negative-Biased, N-Gram) ===\")\n",
    "pd.DataFrame(results_ngram_negBias).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class NEG Bias - NGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Nâ€‘Gram models (Negative-Biased)\n",
    "models_negBias_ngram = {\n",
    "    \"DecisionTree\": dt_best_negBias_ngram,\n",
    "    \"SVM\": svm_best_negBias_ngram,\n",
    "    \"RandomForest\": rf_best_negBias_ngram,\n",
    "    \"kNN\": knn_best_negBias_ngram,\n",
    "    \"NaiveBayes\": nb_best_negBias_ngram\n",
    "}\n",
    "\n",
    "for model_name, model in models_negBias_ngram.items():\n",
    "    # X_test_ngram_negBias: your Nâ€‘Gram test data (Negative-Biased)\n",
    "    # y_test_negBias:       your Negative-Biased labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_ngram_negBias)\n",
    "    cm = confusion_matrix(y_test_negBias, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Negative-Biased, N-Gram)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”´ Classification Dual-Class (Positive Biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posBias = pd.read_csv('Datasets/preprocessed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posBias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 3 Star Reviews with POSITIVE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop the existing 'Sentiment' column (ensuring no conflicts)\n",
    "df_posBias.drop(columns=['Sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posBias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a function to merge 1,2 â†’ negative and 3,4,5 â†’ positive\n",
    "def map_pos_biased(score):\n",
    "    if score in [1, 2]:\n",
    "        return 'negative'\n",
    "    else:  # 3, 4, 5 become positive\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a new 'Sentiment' column with updated mapping\n",
    "df_posBias['Sentiment'] = df_posBias['Score'].apply(map_pos_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Verify the new class distribution\n",
    "print(\"\\nDistribution for Positive-Biased Classification:\")\n",
    "print(df_posBias['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Features (Preprocessed Text) and Target (Pos-Biased Sentiment)\n",
    "X_pos = df_posBias['Preprocessed_Review']\n",
    "y_pos = df_posBias['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train/Test Split\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
    "    X_pos,\n",
    "    y_pos,\n",
    "    test_size=0.3,      # 70 train / 30 test\n",
    "    random_state=42,\n",
    "    stratify=y_pos\n",
    ")\n",
    "\n",
    "# 8. Print distribution in train/test sets\n",
    "print(\"\\nTraining Set Distribution:\")\n",
    "print(y_train_pos.value_counts())\n",
    "print(\"\\nTest Set Distribution:\")\n",
    "print(y_test_pos.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 1:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Here, we run each model separatelyâ€”each with its own hyperparameter tuning and 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method : TFIDF Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED SENTIMENT: TF-IDF SETUP\n",
    "# ================================\n",
    "print(\"\\n=== Feature Extraction: TF-IDF for Positive-Biased Sentiment ===\")\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer_posBias = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit to 10,000 most important words\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    max_df=0.8,  # Ignore terms that appear in more than 80% of documents\n",
    "    sublinear_tf=True  # Apply sublinear term frequency scaling\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test sets\n",
    "X_train_tfidf_pos = tfidf_vectorizer_posBias.fit_transform(X_train_pos)\n",
    "X_test_tfidf_pos = tfidf_vectorizer_posBias.transform(X_test_pos)\n",
    "\n",
    "# Create a dictionary to store results for this section\n",
    "results_posBias = {}\n",
    "\n",
    "print(\"\\nTF-IDF transformation complete for Positive-Biased Sentiment Classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED SENTIMENT: DECISION TREE\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POSITIVE-BIASED: Decision Tree Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "dt_classifier_posBias = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_posBias = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 10-fold cross-validation\n",
    "dt_grid_search_posBias = GridSearchCV(\n",
    "    estimator=dt_classifier_posBias,\n",
    "    param_grid=dt_param_grid_posBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,  # Adjust CPU usage\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the TF-IDF transformed training data\n",
    "dt_grid_search_posBias.fit(X_train_tfidf_pos, y_train_pos)\n",
    "\n",
    "# Display the best parameters and cross-validation score\n",
    "print(\"\\nBest Params (Positive-Biased, Decision Tree):\", dt_grid_search_posBias.best_params_)\n",
    "print(\"Best CV Score (Positive-Biased, Decision Tree):\", dt_grid_search_posBias.best_score_)\n",
    "\n",
    "# Predict on test data using the best model\n",
    "dt_best_posBias = dt_grid_search_posBias.best_estimator_\n",
    "y_pred_dt_pos = dt_best_posBias.predict(X_test_tfidf_pos)\n",
    "\n",
    "# Evaluate performance\n",
    "dt_test_accuracy_pos = accuracy_score(y_test_pos, y_pred_dt_pos)\n",
    "print(\"\\nTest Accuracy (Positive-Biased, Decision Tree):\", dt_test_accuracy_pos)\n",
    "print(classification_report(y_test_pos, y_pred_dt_pos))\n",
    "\n",
    "# Store results\n",
    "results_posBias['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_posBias.best_params_,\n",
    "    'best_cv_score': dt_grid_search_posBias.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_pos,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_dt_pos, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "dt_cv_results_posBias = pd.DataFrame(dt_grid_search_posBias.cv_results_)\n",
    "dt_cv_results_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_decisionTree_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDecision Tree Model Training & Evaluation Complete for Positive-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: VALIDATION CURVE (max_depth)\n",
    "# ================================\n",
    "\n",
    "param_range = [5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    dt_classifier_posBias, X_train_tfidf_pos, y_train_pos, \n",
    "    param_name=\"max_depth\", param_range=param_range,\n",
    "    scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range, train_mean, label=\"Training Score\", color='blue', marker='o')\n",
    "plt.plot(param_range, test_mean, label=\"Validation Score\", color='green', linestyle='--', marker='x')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title(\"Validation Curve for max_depth (Positive-Biased)\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: CONFUSION MATRIX\n",
    "# ================================\n",
    "\n",
    "cm = confusion_matrix(y_test_pos, y_pred_dt_pos)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix: Decision Tree (Positive-Biased)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: HYPERPARAMETER TUNING RESULTS\n",
    "# ================================\n",
    "\n",
    "# Convert results to DataFrame\n",
    "dt_cv_results_df_posBias = pd.DataFrame(dt_grid_search_posBias.cv_results_)\n",
    "\n",
    "# Display top rows\n",
    "print(\"\\n--- Decision Tree: Cross-Validation Scores for Each Fold (Positive-Biased) ---\")\n",
    "print(dt_cv_results_df_posBias[['params', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "# Save results to CSV\n",
    "dt_cv_results_df_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_decisionTree_cv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: LINEAR SVM\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TF-IDF: Linear SVM (Positive-Biased Sentiment)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Define the model\n",
    "svm_classifier_posBias = LinearSVC(max_iter=10000)\n",
    "\n",
    "# 2. Define the hyperparameter grid\n",
    "svm_param_grid_posBias = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]  # Regularization parameter\n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV for hyperparameter tuning\n",
    "svm_grid_search_posBias = GridSearchCV(\n",
    "    estimator=svm_classifier_posBias,\n",
    "    param_grid=svm_param_grid_posBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold Cross Validation\n",
    "    n_jobs=4,  # Parallel processing\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# 4. Train the model using GridSearchCV\n",
    "svm_grid_search_posBias.fit(X_train_tfidf_pos, y_train_pos)\n",
    "\n",
    "# 5. Display the best hyperparameters\n",
    "print(\"\\nBest Params (TF-IDF, SVM - Positive-Biased):\", svm_grid_search_posBias.best_params_)\n",
    "print(\"Best CV Score (TF-IDF, SVM - Positive-Biased):\", svm_grid_search_posBias.best_score_)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "svm_best_posBias = svm_grid_search_posBias.best_estimator_\n",
    "y_pred_svm_pos = svm_best_posBias.predict(X_test_tfidf_pos)\n",
    "\n",
    "# 7. Evaluate the model performance\n",
    "svm_test_accuracy_posBias = accuracy_score(y_test_pos, y_pred_svm_pos)\n",
    "print(\"Test Accuracy (TF-IDF, SVM - Positive-Biased):\", svm_test_accuracy_posBias)\n",
    "print(classification_report(y_test_pos, y_pred_svm_pos))\n",
    "\n",
    "# 8. Store results in dictionary\n",
    "results_posBias['SVM'] = {\n",
    "    'best_params': svm_grid_search_posBias.best_params_,\n",
    "    'best_cv_score': svm_grid_search_posBias.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_posBias,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_svm_pos, output_dict=True)\n",
    "}\n",
    "\n",
    "# 9. Save cross-validation results to CSV\n",
    "svm_cv_results_posBias = pd.DataFrame(svm_grid_search_posBias.cv_results_)\n",
    "svm_cv_results_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_svm_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nLinear SVM Model Training & Evaluation Complete for Positive-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: RANDOM FOREST\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TF-IDF: Random Forest (Positive-Biased Sentiment)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Define the model\n",
    "rf_classifier_posBias = RandomForestClassifier()\n",
    "\n",
    "# 2. Define the hyperparameter grid\n",
    "rf_param_grid_posBias = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'max_features': ['sqrt', 'log2']  # Features considered for best split\n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV for hyperparameter tuning\n",
    "rf_grid_search_posBias = GridSearchCV(\n",
    "    estimator=rf_classifier_posBias,\n",
    "    param_grid=rf_param_grid_posBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold Cross Validation\n",
    "    n_jobs=4,  # Parallel processing\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# 4. Train the model using GridSearchCV\n",
    "rf_grid_search_posBias.fit(X_train_tfidf_pos, y_train_pos)\n",
    "\n",
    "# 5. Display the best hyperparameters\n",
    "print(\"\\nBest Params (TF-IDF, Random Forest - Positive-Biased):\", rf_grid_search_posBias.best_params_)\n",
    "print(\"Best CV Score (TF-IDF, Random Forest - Positive-Biased):\", rf_grid_search_posBias.best_score_)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "rf_best_posBias = rf_grid_search_posBias.best_estimator_\n",
    "y_pred_rf_pos = rf_best_posBias.predict(X_test_tfidf_pos)\n",
    "\n",
    "# 7. Evaluate the model performance\n",
    "rf_test_accuracy_posBias = accuracy_score(y_test_pos, y_pred_rf_pos)\n",
    "print(\"Test Accuracy (TF-IDF, Random Forest - Positive-Biased):\", rf_test_accuracy_posBias)\n",
    "print(classification_report(y_test_pos, y_pred_rf_pos))\n",
    "\n",
    "# 8. Store results in dictionary\n",
    "results_posBias['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_posBias.best_params_,\n",
    "    'best_cv_score': rf_grid_search_posBias.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_posBias,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_rf_pos, output_dict=True)\n",
    "}\n",
    "\n",
    "# 9. Save cross-validation results to CSV\n",
    "rf_cv_results_posBias = pd.DataFrame(rf_grid_search_posBias.cv_results_)\n",
    "rf_cv_results_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_randomForest_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nRandom Forest Model Training & Evaluation Complete for Positive-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: k-NEAREST NEIGHBORS (kNN)\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TF-IDF: kNN (Positive-Biased Sentiment)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Define the model\n",
    "knn_classifier_posBias = KNeighborsClassifier()\n",
    "\n",
    "# 2. Define the hyperparameter grid\n",
    "knn_param_grid_posBias = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider\n",
    "    'weights': ['uniform', 'distance'],  # Weight function\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']  # Distance metric\n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV for hyperparameter tuning\n",
    "knn_grid_search_posBias = GridSearchCV(\n",
    "    estimator=knn_classifier_posBias,\n",
    "    param_grid=knn_param_grid_posBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold Cross Validation\n",
    "    n_jobs=4,  # Parallel processing\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# 4. Train the model using GridSearchCV\n",
    "knn_grid_search_posBias.fit(X_train_tfidf_pos, y_train_pos)\n",
    "\n",
    "# 5. Display the best hyperparameters\n",
    "print(\"\\nBest Params (TF-IDF, kNN - Positive-Biased):\", knn_grid_search_posBias.best_params_)\n",
    "print(\"Best CV Score (TF-IDF, kNN - Positive-Biased):\", knn_grid_search_posBias.best_score_)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "knn_best_posBias = knn_grid_search_posBias.best_estimator_\n",
    "y_pred_knn_pos = knn_best_posBias.predict(X_test_tfidf_pos)\n",
    "\n",
    "# 7. Evaluate the model performance\n",
    "knn_test_accuracy_posBias = accuracy_score(y_test_pos, y_pred_knn_pos)\n",
    "print(\"Test Accuracy (TF-IDF, kNN - Positive-Biased):\", knn_test_accuracy_posBias)\n",
    "print(classification_report(y_test_pos, y_pred_knn_pos))\n",
    "\n",
    "# 8. Store results in dictionary\n",
    "results_posBias['kNN'] = {\n",
    "    'best_params': knn_grid_search_posBias.best_params_,\n",
    "    'best_cv_score': knn_grid_search_posBias.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_posBias,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_knn_pos, output_dict=True)\n",
    "}\n",
    "\n",
    "# 9. Save cross-validation results to CSV\n",
    "knn_cv_results_posBias = pd.DataFrame(knn_grid_search_posBias.cv_results_)\n",
    "knn_cv_results_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_knn_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nkNN Model Training & Evaluation Complete for Positive-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: NAÃVE BAYES (TF-IDF)\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TF-IDF: NaÃ¯ve Bayes (Positive-Biased Sentiment)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Define the model\n",
    "nb_classifier_posBias = MultinomialNB()\n",
    "\n",
    "# 2. Define the hyperparameter grid\n",
    "nb_param_grid_posBias = {\n",
    "    'alpha': [0.5, 1.0, 1.5]  # Smoothing parameter\n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV for hyperparameter tuning\n",
    "nb_grid_search_posBias = GridSearchCV(\n",
    "    estimator=nb_classifier_posBias,\n",
    "    param_grid=nb_param_grid_posBias,\n",
    "    scoring='accuracy',\n",
    "    cv=10,  # 10-fold Cross Validation\n",
    "    n_jobs=4,  # Parallel processing\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# 4. Train the model using GridSearchCV\n",
    "nb_grid_search_posBias.fit(X_train_tfidf_pos, y_train_pos)\n",
    "\n",
    "# 5. Display the best hyperparameters\n",
    "print(\"\\nBest Params (TF-IDF, NaÃ¯ve Bayes - Positive-Biased):\", nb_grid_search_posBias.best_params_)\n",
    "print(\"Best CV Score (TF-IDF, NaÃ¯ve Bayes - Positive-Biased):\", nb_grid_search_posBias.best_score_)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "nb_best_posBias = nb_grid_search_posBias.best_estimator_\n",
    "y_pred_nb_pos = nb_best_posBias.predict(X_test_tfidf_pos)\n",
    "\n",
    "# 7. Evaluate the model performance\n",
    "nb_test_accuracy_posBias = accuracy_score(y_test_pos, y_pred_nb_pos)\n",
    "print(\"Test Accuracy (TF-IDF, NaÃ¯ve Bayes - Positive-Biased):\", nb_test_accuracy_posBias)\n",
    "print(classification_report(y_test_pos, y_pred_nb_pos))\n",
    "\n",
    "# 8. Store results in dictionary\n",
    "results_posBias['NaÃ¯veBayes'] = {\n",
    "    'best_params': nb_grid_search_posBias.best_params_,\n",
    "    'best_cv_score': nb_grid_search_posBias.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_posBias,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_nb_pos, output_dict=True)\n",
    "}\n",
    "\n",
    "# 9. Save cross-validation results to CSV\n",
    "nb_cv_results_posBias = pd.DataFrame(nb_grid_search_posBias.cv_results_)\n",
    "nb_cv_results_posBias.to_csv('Results_2C(PosBias)/TFIDF_Models/tfidf_posBias_naiveBayes_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nNaÃ¯ve Bayes Model Training & Evaluation Complete for Positive-Biased Sentiment!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# SUMMARY: TF-IDF (Positive-Biased Sentiment)\n",
    "# ================================\n",
    "print(\"\\n=== SUMMARY: TF-IDF Classification for Positive-Biased Sentiment ===\")\n",
    "\n",
    "# Convert results dictionary into a DataFrame for better visualization\n",
    "summary_posBias_tfidf = pd.DataFrame(results_posBias).T  # Transpose for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class POS Bias - TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best TFâ€‘IDF models (Positive-Biased)\n",
    "models_posBias_tfidf = {\n",
    "    \"DecisionTree\": dt_best_posBias,\n",
    "    \"SVM\": svm_best_posBias,\n",
    "    \"RandomForest\": rf_best_posBias,\n",
    "    \"kNN\": knn_best_posBias,\n",
    "    \"NaiveBayes\": nb_best_posBias\n",
    "}\n",
    "\n",
    "for model_name, model in models_posBias_tfidf.items():\n",
    "    # X_test_tfidf_pos: your TFâ€‘IDF test data (Positive-Biased)\n",
    "    # y_test_pos:       your Positive-Biased labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_tfidf_pos)\n",
    "    cm = confusion_matrix(y_test_pos, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Positive-Biased, TF-IDF)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Method 2:** N-Gram (Tri-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method: N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# POSITIVE-BIASED: N-Gram Feature Extraction\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POSITIVE-BIASED: N-Gram Feature Extraction\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for N-Grams (unigrams, bigrams, trigrams)\n",
    "ngram_vectorizer_pos = CountVectorizer(\n",
    "    ngram_range=(1, 3),  # (1,3) includes unigrams, bigrams, and trigrams\n",
    "    max_features=10000,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the positive-biased training set and transform both training and test sets\n",
    "X_train_ngram_pos = ngram_vectorizer_pos.fit_transform(X_train_pos)\n",
    "X_test_ngram_pos = ngram_vectorizer_pos.transform(X_test_pos)\n",
    "\n",
    "# (Optional) Create a dictionary to store the results for the N-Gram approach for Positive-Biased sentiment\n",
    "results_ngram_pos = {}\n",
    "\n",
    "print(\"\\nN-Gram Feature Extraction Complete for Positive-Biased Sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  POSITIVE-BIASED: Decision Tree (N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define Decision Tree model\n",
    "dt_classifier_pos_ngram = DecisionTreeClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "dt_param_grid_pos_ngram = {\n",
    "    'max_depth': [10, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "dt_grid_search_pos_ngram = GridSearchCV(\n",
    "    estimator=dt_classifier_pos_ngram,\n",
    "    param_grid=dt_param_grid_pos_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Decision Tree (Positive-Biased)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit GridSearch on the N-Gram transformed training data\n",
    "dt_grid_search_pos_ngram.fit(X_train_ngram_pos, y_train_pos)\n",
    "\n",
    "# Best parameters and cross-validation score\n",
    "print(\"\\nBest Params (N-Gram, Decision Tree - Positive Biased):\", dt_grid_search_pos_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Decision Tree - Positive Biased):\", dt_grid_search_pos_ngram.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "dt_best_pos_ngram = dt_grid_search_pos_ngram.best_estimator_\n",
    "y_pred_dt_pos_ngram = dt_best_pos_ngram.predict(X_test_ngram_pos)\n",
    "\n",
    "# Evaluate model\n",
    "dt_test_accuracy_pos_ngram = accuracy_score(y_test_pos, y_pred_dt_pos_ngram)\n",
    "print(\"Test Accuracy (N-Gram, Decision Tree - Positive Biased):\", dt_test_accuracy_pos_ngram)\n",
    "print(classification_report(y_test_pos, y_pred_dt_pos_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_pos['DecisionTree'] = {\n",
    "    'best_params': dt_grid_search_pos_ngram.best_params_,\n",
    "    'best_cv_score': dt_grid_search_pos_ngram.best_score_,\n",
    "    'test_accuracy': dt_test_accuracy_pos_ngram,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_dt_pos_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "dt_cv_results_pos_ngram = pd.DataFrame(dt_grid_search_pos_ngram.cv_results_)\n",
    "dt_cv_results_pos_ngram.to_csv('Results_2C(PosBias)/NGram_Models/ngram_posBias_decisionTree_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDecision Tree Model Training & Evaluation Complete for Positive-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  POSITIVE-BIASED: Linear SVM (N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define SVM model\n",
    "svm_classifier_pos_ngram = LinearSVC(max_iter=10000)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid_pos_ngram = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "svm_grid_search_pos_ngram = GridSearchCV(\n",
    "    estimator=svm_classifier_pos_ngram,\n",
    "    param_grid=svm_param_grid_pos_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Linear SVM (Positive-Biased)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit GridSearch on the N-Gram transformed training data\n",
    "svm_grid_search_pos_ngram.fit(X_train_ngram_pos, y_train_pos)\n",
    "\n",
    "# Best parameters and cross-validation score\n",
    "print(\"\\nBest Params (N-Gram, SVM - Positive Biased):\", svm_grid_search_pos_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, SVM - Positive Biased):\", svm_grid_search_pos_ngram.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "svm_best_pos_ngram = svm_grid_search_pos_ngram.best_estimator_\n",
    "y_pred_svm_pos_ngram = svm_best_pos_ngram.predict(X_test_ngram_pos)\n",
    "\n",
    "# Evaluate model\n",
    "svm_test_accuracy_pos_ngram = accuracy_score(y_test_pos, y_pred_svm_pos_ngram)\n",
    "print(\"Test Accuracy (N-Gram, SVM - Positive Biased):\", svm_test_accuracy_pos_ngram)\n",
    "print(classification_report(y_test_pos, y_pred_svm_pos_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_pos['SVM'] = {\n",
    "    'best_params': svm_grid_search_pos_ngram.best_params_,\n",
    "    'best_cv_score': svm_grid_search_pos_ngram.best_score_,\n",
    "    'test_accuracy': svm_test_accuracy_pos_ngram,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_svm_pos_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "svm_cv_results_pos_ngram = pd.DataFrame(svm_grid_search_pos_ngram.cv_results_)\n",
    "svm_cv_results_pos_ngram.to_csv('Results_2C(PosBias)/NGram_Models/ngram_posBias_svm_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nLinear SVM Model Training & Evaluation Complete for Positive-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  POSITIVE-BIASED: Random Forest (N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_classifier_pos_ngram = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid_pos_ngram = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']  # Usually beneficial for text\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "rf_grid_search_pos_ngram = GridSearchCV(\n",
    "    estimator=rf_classifier_pos_ngram,\n",
    "    param_grid=rf_param_grid_pos_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: Random Forest (Positive-Biased)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit GridSearch on the N-Gram transformed training data\n",
    "rf_grid_search_pos_ngram.fit(X_train_ngram_pos, y_train_pos)\n",
    "\n",
    "# Best parameters and cross-validation score\n",
    "print(\"\\nBest Params (N-Gram, Random Forest - Positive Biased):\", rf_grid_search_pos_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, Random Forest - Positive Biased):\", rf_grid_search_pos_ngram.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "rf_best_pos_ngram = rf_grid_search_pos_ngram.best_estimator_\n",
    "y_pred_rf_pos_ngram = rf_best_pos_ngram.predict(X_test_ngram_pos)\n",
    "\n",
    "# Evaluate model\n",
    "rf_test_accuracy_pos_ngram = accuracy_score(y_test_pos, y_pred_rf_pos_ngram)\n",
    "print(\"Test Accuracy (N-Gram, Random Forest - Positive Biased):\", rf_test_accuracy_pos_ngram)\n",
    "print(classification_report(y_test_pos, y_pred_rf_pos_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_pos['RandomForest'] = {\n",
    "    'best_params': rf_grid_search_pos_ngram.best_params_,\n",
    "    'best_cv_score': rf_grid_search_pos_ngram.best_score_,\n",
    "    'test_accuracy': rf_test_accuracy_pos_ngram,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_rf_pos_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "rf_cv_results_pos_ngram = pd.DataFrame(rf_grid_search_pos_ngram.cv_results_)\n",
    "rf_cv_results_pos_ngram.to_csv('Results_2C(PosBias)/NGram_Models/ngram_posBias_randomForest_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nRandom Forest Model Training & Evaluation Complete for Positive-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M4 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  POSITIVE-BIASED: kNN (N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define kNN model\n",
    "knn_classifier_pos_ngram = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "knn_param_grid_pos_ngram = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "knn_grid_search_pos_ngram = GridSearchCV(\n",
    "    estimator=knn_classifier_pos_ngram,\n",
    "    param_grid=knn_param_grid_pos_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: kNN (Positive-Biased)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit GridSearch on the N-Gram transformed training data\n",
    "knn_grid_search_pos_ngram.fit(X_train_ngram_pos, y_train_pos)\n",
    "\n",
    "# Best parameters and cross-validation score\n",
    "print(\"\\nBest Params (N-Gram, kNN - Positive Biased):\", knn_grid_search_pos_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, kNN - Positive Biased):\", knn_grid_search_pos_ngram.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "knn_best_pos_ngram = knn_grid_search_pos_ngram.best_estimator_\n",
    "y_pred_knn_pos_ngram = knn_best_pos_ngram.predict(X_test_ngram_pos)\n",
    "\n",
    "# Evaluate model\n",
    "knn_test_accuracy_pos_ngram = accuracy_score(y_test_pos, y_pred_knn_pos_ngram)\n",
    "print(\"Test Accuracy (N-Gram, kNN - Positive Biased):\", knn_test_accuracy_pos_ngram)\n",
    "print(classification_report(y_test_pos, y_pred_knn_pos_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_pos['kNN'] = {\n",
    "    'best_params': knn_grid_search_pos_ngram.best_params_,\n",
    "    'best_cv_score': knn_grid_search_pos_ngram.best_score_,\n",
    "    'test_accuracy': knn_test_accuracy_pos_ngram,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_knn_pos_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "knn_cv_results_pos_ngram = pd.DataFrame(knn_grid_search_pos_ngram.cv_results_)\n",
    "knn_cv_results_pos_ngram.to_csv('Results_2C(PosBias)/NGram_Models/ngram_posBias_knn_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nkNN Model Training & Evaluation Complete for Positive-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "#  POSITIVE-BIASED: NaÃ¯ve Bayes (N-Gram)\n",
    "# -------------------------------\n",
    "\n",
    "# Define NaÃ¯ve Bayes model\n",
    "nb_classifier_pos_ngram = MultinomialNB()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "nb_param_grid_pos_ngram = {\n",
    "    'alpha': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "nb_grid_search_pos_ngram = GridSearchCV(\n",
    "    estimator=nb_classifier_pos_ngram,\n",
    "    param_grid=nb_param_grid_pos_ngram,\n",
    "    scoring='accuracy',\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"N-Gram: NaÃ¯ve Bayes (Positive-Biased)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit GridSearch on the N-Gram transformed training data\n",
    "nb_grid_search_pos_ngram.fit(X_train_ngram_pos, y_train_pos)\n",
    "\n",
    "# Best parameters and cross-validation score\n",
    "print(\"\\nBest Params (N-Gram, NaÃ¯ve Bayes - Positive Biased):\", nb_grid_search_pos_ngram.best_params_)\n",
    "print(\"Best CV Score (N-Gram, NaÃ¯ve Bayes - Positive Biased):\", nb_grid_search_pos_ngram.best_score_)\n",
    "\n",
    "# Predict on test data\n",
    "nb_best_pos_ngram = nb_grid_search_pos_ngram.best_estimator_\n",
    "y_pred_nb_pos_ngram = nb_best_pos_ngram.predict(X_test_ngram_pos)\n",
    "\n",
    "# Evaluate model\n",
    "nb_test_accuracy_pos_ngram = accuracy_score(y_test_pos, y_pred_nb_pos_ngram)\n",
    "print(\"Test Accuracy (N-Gram, NaÃ¯ve Bayes - Positive Biased):\", nb_test_accuracy_pos_ngram)\n",
    "print(classification_report(y_test_pos, y_pred_nb_pos_ngram))\n",
    "\n",
    "# Store results\n",
    "results_ngram_pos['NaiveBayes'] = {\n",
    "    'best_params': nb_grid_search_pos_ngram.best_params_,\n",
    "    'best_cv_score': nb_grid_search_pos_ngram.best_score_,\n",
    "    'test_accuracy': nb_test_accuracy_pos_ngram,\n",
    "    'classification_report': classification_report(y_test_pos, y_pred_nb_pos_ngram, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save cross-validation results to CSV\n",
    "nb_cv_results_pos_ngram = pd.DataFrame(nb_grid_search_pos_ngram.cv_results_)\n",
    "nb_cv_results_pos_ngram.to_csv('Results_2C(PosBias)/NGram_Models/ngram_posBias_naiveBayes_cv_results.csv', index=False)\n",
    "\n",
    "print(\"\\nNaÃ¯ve Bayes Model Training & Evaluation Complete for Positive-Biased Sentiment (N-Gram)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results_ngram_pos = pd.DataFrame(results_ngram_pos).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices (Dual Class POS Bias - NGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Best Nâ€‘Gram models (Positive-Biased)\n",
    "models_posBias_ngram = {\n",
    "    \"DecisionTree\": dt_best_pos_ngram,\n",
    "    \"SVM\": svm_best_pos_ngram,\n",
    "    \"RandomForest\": rf_best_pos_ngram,\n",
    "    \"kNN\": knn_best_pos_ngram,\n",
    "    \"NaiveBayes\": nb_best_pos_ngram\n",
    "}\n",
    "\n",
    "for model_name, model in models_posBias_ngram.items():\n",
    "    # X_test_ngram_pos: your Nâ€‘Gram test data (Positive-Biased)\n",
    "    # y_test_pos:       your Positive-Biased labels (negative, positive)\n",
    "    y_pred = model.predict(X_test_ngram_pos)\n",
    "    cm = confusion_matrix(y_test_pos, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {model_name} (Positive-Biased, N-Gram)\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
